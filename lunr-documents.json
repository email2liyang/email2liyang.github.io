[{"content":"EKS Cluster is a fleet of ec2 instances which provide the underlying compute power, when we schedule a pod, the first thing it need to do is to pull docker image and save it into ec2 host's root disk. we could not predict how many pods and what kind of pods is going to schedule on the ec2 node , so it's essential to provision enough root diskspace for each ec2 host we learn this lesson hard by painful issues, our eks production is mainly for data processing and we suddenly having a lot of pod eviction issues, if we describe the pod, k8s will report disk pressure and start to evict pod on the ec2 host. initially we think it maybe caused by we output too many logs or download too many intermediate file without clean up, but in the end it's not that case. it's caused by our docker image is too large and ec2 root disk is too small, after we expend the ec2 root disk from 20G to 200G, the eviction issue gone away totally. I explain this to the team like below, imagine you have a 2 bed rooms house, but there are 10 people try to sleep in, you have to kick off extra people to make sure your room could only host certain number of people , this is how the eviction issue happen, if you want to host all the 10 people , you have to expend your house to build more bed rooms, so that all 10 people could sleep in your house.","path":"infra/kube/make-sure-to-have-enough-root-disk-space-for-ec2-instnace-inside-eks-cluster.html","title":"make sure to have enough root disk space for ec2 instnace inside EKS cluster"},{"content":"AWS Lambda provide serverless runtime, lots of developers love this service. we could use many languages on aws lambda, e.g: nodejs, python, java etc. but what if we have some customised c library, or custom utility package to install into the runtime env ? now AWS lambda support deploy docker image to lambda env The docker image must implement Runtime API, AWS already provided base images for lots of programming languages to extend , so we only need to develop logic and starting from the based image to customise the docker image. take python image as an example, reference from https://docs.aws.amazon.com/lambda/latest/dg/images-create.html FROM public.ecr.aws/lambda/python:3.8 # Copy function code COPY app.py ${LAMBDA_TASK_ROOT} # Install the function's dependencies using file requirements.txt # from your project folder. COPY requirements.txt . RUN pip3 install -r requirements.txt --target &quot;${LAMBDA_TASK_ROOT}&quot; # Set the CMD to your handler (could also be done as a parameter override outside of the Dockerfile) CMD [ &quot;app.handler&quot; ] we could use AWS Serverless Application Model to manage the build and deployment. a sample config file could like below AWSTemplateFormatVersion: '2010-09-09' Transform: AWS::Serverless-2016-10-31 Description: &gt; python3.8 SAM Template for xxx # More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst Globals: Function: Timeout: 600 Resources: SampleFunction: Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction Properties: PackageType: Image Environment: Variables: LOG_LEVEL: INFO Metadata: Dockerfile: Dockerfile DockerContext: . DockerTag: v1 Outputs: SampleFunction: Description: &quot;xxx Lambda Function ARN&quot; Value: !GetAtt SampleFunction.Arn we could use command sam build to build the docker image and use command sam local invoke SearchablePdfFunction --event events/sample_request.json to trigger the lambda locally we could see log like below REPORT RequestId: e207ea8b-fe68-4249-bfef-181f4f0b3098 Init Duration: 1.07 ms Duration: 125.20 ms Billed Duration: 200 ms Memory Size: 128 MB Max Memory Used: 128 MB &quot;Hello from AWS Lambda using Python3.8.11 (default, Jul 14 2021, 13:00:16) \\n[GCC 7.3.1 20180712 (Red Hat 7.3.1-13)]!&quot;% sometimes we may want to use our own images and customise the image, we could also install the lambda runtime client manually to make it work, for example, I want to use OCRMyPDF docker image to process pdf files, we could do the following # Define global args ARG FUNCTION_DIR=&quot;/home/app/&quot; ARG RUNTIME_VERSION=&quot;3.9&quot; # Stage 1 - bundle base image + runtime # Grab a fresh copy of the image and install GCC FROM jbarlow83/ocrmypdf AS Image-Base # Install GCC (Alpine uses musl but we compile and link dependencies with GCC) RUN apt-get update &amp;&amp; apt-get install -y python3-distutils python3-dev default-libmysqlclient-dev build-essential # Install aws-lambda-cpp build dependencies RUN apt-get install -y \\ g++ \\ make \\ cmake \\ unzip \\ libcurl4-openssl-dev # (Optional) Add Lambda Runtime Interface Emulator and use a script in the ENTRYPOINT for simpler local runs ADD https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie /usr/bin/aws-lambda-rie RUN chmod 755 /usr/bin/aws-lambda-rie # Stage 2 - build function and dependencies FROM Image-Base AS build-image #Include global args in this stage of the build ARG FUNCTION_DIR ARG RUNTIME_VERSION # Create function directory RUN mkdir -p ${FUNCTION_DIR} # Copy handler function COPY *.py requirements.txt ${FUNCTION_DIR} # Optional – Install the function's dependencies # RUN python${RUNTIME_VERSION} -m pip install -r requirements.txt --target ${FUNCTION_DIR} # Install Lambda Runtime Interface Client for Python RUN python${RUNTIME_VERSION} -m pip install awslambdaric --target ${FUNCTION_DIR} # Optional – Install the function's dependencies RUN python${RUNTIME_VERSION} -m pip install -r ${FUNCTION_DIR}requirements.txt --target ${FUNCTION_DIR} # Stage 3 - final runtime image # Grab a fresh copy of the Python image FROM Image-Base # Include global arg in this stage of the build ARG FUNCTION_DIR # Set working directory to function root directory WORKDIR ${FUNCTION_DIR} # Copy in the built dependencies COPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR} COPY entry.sh / RUN chmod 755 /entry.sh ENTRYPOINT [ &quot;/entry.sh&quot; ] CMD [ &quot;app.handler&quot; ] the entry.sh is used to switch between local dev env and lambda env #!/bin/sh if [ -z &quot;${AWS_LAMBDA_RUNTIME_API}&quot; ]; then exec /usr/bin/aws-lambda-rie /usr/bin/python3 -m awslambdaric $1 else exec /usr/bin/python3 -m awslambdaric $1 fi ","path":"infra/aws/put-docker-container-into-aws-lambda.html","title":"Put Docker Container into AWS Lambda"},{"content":"Recently I need to design a way to store and transfer sensitive data in a secure way, as the data is sensitive, no one could see the data except the owner, our system is only granted to get the plain text at run time, when storing or transferring the data, it must remains encrypted and the password for encryption should be encrypted as well, we also requested not use any hard coded password to do encryption and decryption there are 2 types of encryption mechanisms Symmetric encryption and Asymmetric encryption, Symmetric encryption uses a private key to encrypt and decrypt, Asymmetric encryption uses the public key of the recipient to encrypt the message. Then if the recipient wants to decrypt the message the recipient will have to use his/her private key to decrypt, we could tell, Asymmetric encryption is safer than Symmetric encryption, but it's not as efficient as Symmetric encryption and only suitable for small amount of data (e.g: key or password) encryption. https protocol is using the same way , it's using Asymmetric encryption to exchange the password, then use Symmetric encryption to exchange the actual content. I would like to do the same. for Asymmetric encryption, we use RSA algorithm, for Symmetric encryption, we use AES-256 algorithm. end user could provide us a pubkey and we could use the pubkey to encrypt any AES password and iv, for the actual content , for actual content, we could use AES-256 algorithm to encrypt the actual content and deliver &quot;rsa_encrypted_aes_password , rsa_encrypted_aes_iv , aes_encrypted_data&quot; to end user, when end user get the data, they could use their private key to decrypt rsa_encrypted_aes_password and rsa_encrypted_aes_iv, then use them to decrypt aes_encrypted_data. pycryptodome provide out of box support for both RSA and AES algorithm for RSA encryption and decryption, we could use code like below from Crypto.Cipher import PKCS1_OAEP from Crypto.PublicKey import RSA def rsa_encrypt(original_data: bytes) -&gt; bytes: &quot;&quot;&quot; use cust_key to encrypt the data :return: base64 encoded encrypted str &quot;&quot;&quot; rsa_public_key = RSA.import_key(get_pubkey()) cipher_rsa = PKCS1_OAEP.new(rsa_public_key) return cipher_rsa.encrypt(original_data) def rsa_decrypt(encrypted_content: bytes, private_key: bytes) -&gt; bytes: rsa_private_key = RSA.import_key(private_key) cipher_rsa = PKCS1_OAEP.new(rsa_private_key) return cipher_rsa.decrypt(encrypted_content) for AES encryption and decryption, we could use code like below from Crypto.Cipher import AES from Crypto.Util.Padding import pad, unpad def aes_encrypt( original_data: bytes, aes_secret: Optional[bytes] = None, aes_iv: Optional[bytes] = None, ) -&gt; Tuple[bytes, bytes, bytes]: &quot;&quot;&quot; use aes encrypt to encrypt the data :param original_data: :return: turple of (encrypted data in bytes, rsa encrypted aes_secret, rsa encrypted aes_iv &quot;&quot;&quot; # use aes-256-cbc 32*8=256 bit if not aes_secret: aes_secret = os.urandom(32) if not aes_iv: aes_iv = os.urandom(16) cipher = AES.new(aes_secret, AES.MODE_CBC, iv=aes_iv) # use default pkcs7 padding return ( cipher.encrypt(pad(original_data, AES.block_size)), rsa_encrypt(aes_secret), rsa_encrypt(aes_iv), ) def aes_decrypt( encrypted_content: bytes, aes_secret: bytes, aes_iv: bytes ) -&gt; bytes: cipher = AES.new(aes_secret, AES.MODE_CBC, iv=aes_iv) return unpad(cipher.decrypt(encrypted_content), AES.block_size) wait a second, there is 1 misleading point, we got user's pubkey and we need to store it somewhere in a secure way, store pubkey in a clear text may not be a good idea, so we decide to use KMS to encrypt the pubkey and put it into s3, with this we do not have any hard coded password and everything we store/transfer is encrypted by default for KMS encryption and decryption we could use boto3 kms client to test kms, we could use moto , sample kms test could be found https://github.com/spulec/moto/blob/master/tests/test_kms/test_kms_boto3.py","path":"programing/python/encryption-and-decryption-in-python.html","title":"encryption and decryption in python"},{"content":"in previous article, I described how to use sqlalchemy and how to do unit test with python build in db sqlite, sometimes with legacy code, we have to execute raw sql with sqlalchemy engine and some of the raw sql is using database specific functions, to test these logic, we could not spin up sqlite , we have to launch real database. with testcontainers-python, things get really easy. below is the code example from testcontainers-python's webiste to demo how to use it import sqlalchemy from testcontainers.mysql import MySqlContainer with MySqlContainer('mysql:5.7.17') as mysql: engine = sqlalchemy.create_engine(mysql.get_connection_url()) version, = engine.execute(&quot;select version()&quot;).fetchone() print(version) # 5.7.17 in my case, I'm creating engine based on the DB_URL system env, so my first attempt is like below @pytest.fixture(scope=&quot;function&quot;, autouse=True) def init_db(): from testcontainers.mysql import MySqlContainer with MySqlContainer(&quot;mysql:5.7.24&quot;) as mysql: os.environ[&quot;DB_URL&quot;] = mysql.get_connection_url() Base.metadata.create_all(engine_pair) yield os.environ[&quot;DB_URL&quot;] = &quot;sqlite:///:memory:pair&quot; when try the unit test, it failed, and reported it's still using sqlite db rather than mysql. then I figure out my sqlalchemy engine is created in the __init__.py statically, if I want to override it, I must reload the module after system env changed, importlib could help in this context. I need to define the system env DB_URL first, then reload the module to re-create my sqlalchemy engine, after test complete, I could reset system env DB_URL and reload the module again to clean up @pytest.fixture(scope=&quot;function&quot;, autouse=True) def init_db(): from testcontainers.mysql import MySqlContainer with MySqlContainer(&quot;mysql:5.7.24&quot;) as mysql: import os import importlib import db from db import database from db.models import acme_record os.environ[&quot;DB_URL&quot;] = mysql.get_connection_url() importlib.reload(db) importlib.reload(database) importlib.reload(acme_record) from db import engine from db.models import Base Base.metadata.create_all(engine) yield # set env back import os import importlib import db from db import database os.environ[&quot;DB_URL&quot;] = &quot;sqlite:///:memory&quot; importlib.reload(db) importlib.reload(database) ","path":"programing/python/how-to-use-testcontainers-to-test-sqlalchemy-on-real-database.html","title":"How to use testcontainers to test sqlalchemy on real database"},{"content":"Fargate for EKS is great serverless service from AWS, but the nature of the serverless service is ephemeral, and there is no way to run daemon set on Fargate, so no way to run Datadog to collect metrics and log etc. when something goes wrong it will be great if we could still check the log even after the pod and ec2 node has gone. we could mount EFS to the pod running on fargate , if we could write log to EFS system then the information will be still available after pod get destroyed There are already blogs to talk about how to enalbe EFS on Fargate, like https://aws.amazon.com/blogs/aws/new-aws-fargate-for-amazon-eks-now-supports-amazon-efs/, but it's works for newly created eks cluster only, and assume CSI driver already been installed in the cluster, for my case I have an really old EKS cluster which is upgraded to newer version. so the CSI driver may not get installed, we have to install them manually, below is the steps I have to perform to make EFS work on my Fargate for EKS create iam policy for CSI driver save content below to efs-csi-iam-policy.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;elasticfilesystem:DescribeAccessPoints&quot;, &quot;elasticfilesystem:DescribeFileSystems&quot; ], &quot;Resource&quot;: \"*\" }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;elasticfilesystem:CreateAccessPoint&quot; ], &quot;Resource&quot;: \"*\", &quot;Condition&quot;: { &quot;StringLike&quot;: { &quot;aws:RequestTag/efs.csi.aws.com/cluster&quot;: &quot;true&quot; } } }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;elasticfilesystem:DeleteAccessPoint&quot;, &quot;Resource&quot;: \"*\", &quot;Condition&quot;: { &quot;StringEquals&quot;: { &quot;aws:ResourceTag/efs.csi.aws.com/cluster&quot;: &quot;true&quot; } } } ] } create policy by aws iam create-policy \\ --policy-name AmazonEKS_EFS_CSI_Driver_Policy \\ --policy-document file://efs-csi-iam-policy.json create iam role for the CSI driver get OIDC provider aws eks describe-cluster --name acme-Development --query &quot;cluster.identity.oidc.issuer&quot; --output text create role policy, by saving content below to efs-csi-iam-role.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Federated&quot;: &quot;arn:aws:iam::xxxx:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/xxxx&quot; }, &quot;Action&quot;: &quot;sts:AssumeRoleWithWebIdentity&quot;, &quot;Condition&quot;: { &quot;StringEquals&quot;: { &quot;oidc.eks.us-east-1.amazonaws.com/id/xxxx:sub&quot;: &quot;system:serviceaccount:kube-system:efs-csi-controller-sa&quot; } } } ] } create the iam role aws iam create-role \\ --role-name AmazonEKS_EFS_CSI_DriverRole \\ --assume-role-policy-document file://efs-csi-iam-role.json attach policy to role attach-role-policy: aws iam attach-role-policy \\ --policy-arn arn:aws:iam::xxxxx:policy/AmazonEKS_EFS_CSI_Driver_Policy \\ --role-name AmazonEKS_EFS_CSI_DriverRole install CSI driver create service account apiVersion: v1 kind: ServiceAccount metadata: name: efs-csi-controller-sa namespace: kube-system labels: app.kubernetes.io/name: aws-efs-csi-driver annotations: eks.amazonaws.com/role-arn: arn:aws:iam::xxxxx:role/AmazonEKS_EFS_CSI_DriverRole get the driver.yaml as below apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/name: aws-efs-csi-driver name: efs-csi-external-provisioner-role rules: - apiGroups: - &quot;&quot; resources: - persistentvolumes verbs: - get - list - watch - create - delete - apiGroups: - &quot;&quot; resources: - persistentvolumeclaims verbs: - get - list - watch - update - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - list - watch - create - apiGroups: - storage.k8s.io resources: - csinodes verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - list - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - get - watch - list - delete - update - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/name: aws-efs-csi-driver name: efs-csi-provisioner-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: efs-csi-external-provisioner-role subjects: - kind: ServiceAccount name: efs-csi-controller-sa namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: aws-efs-csi-driver name: efs-csi-controller namespace: kube-system spec: replicas: 2 selector: matchLabels: app: efs-csi-controller app.kubernetes.io/instance: kustomize app.kubernetes.io/name: aws-efs-csi-driver template: metadata: labels: app: efs-csi-controller app.kubernetes.io/instance: kustomize app.kubernetes.io/name: aws-efs-csi-driver spec: containers: - args: - --endpoint=$(CSI_ENDPOINT) - --logtostderr - --v=2 - --delete-access-point-root-dir=false env: - name: CSI_ENDPOINT value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock image: 602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver:v1.2.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: healthz initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 3 name: efs-plugin ports: - containerPort: 9909 name: healthz protocol: TCP securityContext: privileged: true volumeMounts: - mountPath: /var/lib/csi/sockets/pluginproxy/ name: socket-dir - args: - --csi-address=$(ADDRESS) - --v=2 - --feature-gates=Topology=true - --leader-election env: - name: ADDRESS value: /var/lib/csi/sockets/pluginproxy/csi.sock image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2 name: csi-provisioner volumeMounts: - mountPath: /var/lib/csi/sockets/pluginproxy/ name: socket-dir - args: - --csi-address=/csi/csi.sock - --health-port=9909 image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2 name: liveness-probe volumeMounts: - mountPath: /csi name: socket-dir hostNetwork: true nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: efs-csi-controller-sa tolerations: - operator: Exists volumes: - emptyDir: {} name: socket-dir --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app.kubernetes.io/name: aws-efs-csi-driver name: efs-csi-node namespace: kube-system spec: selector: matchLabels: app: efs-csi-node app.kubernetes.io/instance: kustomize app.kubernetes.io/name: aws-efs-csi-driver template: metadata: labels: app: efs-csi-node app.kubernetes.io/instance: kustomize app.kubernetes.io/name: aws-efs-csi-driver spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: eks.amazonaws.com/compute-type operator: NotIn values: - fargate containers: - args: - --endpoint=$(CSI_ENDPOINT) - --logtostderr - --v=2 env: - name: CSI_ENDPOINT value: unix:/csi/csi.sock image: 602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver:v1.2.1 livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: healthz initialDelaySeconds: 10 periodSeconds: 2 timeoutSeconds: 3 name: efs-plugin ports: - containerPort: 9809 name: healthz protocol: TCP securityContext: privileged: true volumeMounts: - mountPath: /var/lib/kubelet mountPropagation: Bidirectional name: kubelet-dir - mountPath: /csi name: plugin-dir - mountPath: /var/run/efs name: efs-state-dir - mountPath: /var/amazon/efs name: efs-utils-config - mountPath: /etc/amazon/efs-legacy name: efs-utils-config-legacy - args: - --csi-address=$(ADDRESS) - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH) - --v=2 env: - name: ADDRESS value: /csi/csi.sock - name: DRIVER_REG_SOCK_PATH value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock - name: KUBE_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.1.0-eks-1-18-2 name: csi-driver-registrar volumeMounts: - mountPath: /csi name: plugin-dir - mountPath: /registration name: registration-dir - args: - --csi-address=/csi/csi.sock - --health-port=9809 - --v=2 image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2 name: liveness-probe volumeMounts: - mountPath: /csi name: plugin-dir hostNetwork: true nodeSelector: beta.kubernetes.io/os: linux priorityClassName: system-node-critical tolerations: - operator: Exists volumes: - hostPath: path: /var/lib/kubelet type: Directory name: kubelet-dir - hostPath: path: /var/lib/kubelet/plugins/efs.csi.aws.com/ type: DirectoryOrCreate name: plugin-dir - hostPath: path: /var/lib/kubelet/plugins_registry/ type: Directory name: registration-dir - hostPath: path: /var/run/efs type: DirectoryOrCreate name: efs-state-dir - hostPath: path: /var/amazon/efs type: DirectoryOrCreate name: efs-utils-config - hostPath: path: /etc/amazon/efs type: DirectoryOrCreate name: efs-utils-config-legacy --- apiVersion: storage.k8s.io/v1beta1 kind: CSIDriver metadata: annotations: helm.sh/hook: pre-install, pre-upgrade helm.sh/hook-delete-policy: before-hook-creation helm.sh/resource-policy: keep name: efs.csi.aws.com namespace: kube-system spec: attachRequired: false create the storageclass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: fs-xxxxx # this is the EFS system id could be found from aws console directoryPerms: &quot;700&quot; # gidRangeStart: &quot;1000&quot; # optional # gidRangeEnd: &quot;2000&quot; # optional # basePath: &quot;/dynamic_provisioning&quot; # optional create pv and pvc apiVersion: v1 kind: PersistentVolume metadata: name: efs-pv-flow namespace: flow spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: efs-sc csi: driver: efs.csi.aws.com volumeHandle: fs-xxxx --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: efs-claim-flow namespace: flow spec: accessModes: - ReadWriteMany storageClassName: efs-sc resources: requests: storage: 5Gi create the pod to test EFS apiVersion: v1 kind: Pod metadata: name: efs-app namespace: flow labels: infrastructure: fargate spec: containers: - name: app image: centos command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;while true; do echo $(date -u) &gt;&gt; /data/out; sleep 5; done&quot;] volumeMounts: - name: persistent-storage mountPath: /data volumes: - name: persistent-storage persistentVolumeClaim: claimName: efs-claim-flow I've created an fargate profile to match label infrastructure: fargate on namespace flow to schedule pod on faragte after the pod get created, we could login to the pod to check the file system, the data folder has mounted efs, so we could access efs from pod now! kubectl exec -it efs-app -n flow -- bash [root@efs-app /]# df -h Filesystem Size Used Avail Use% Mounted on overlay 30G 10G 18G 36% / tmpfs 64M 0 64M 0% /dev tmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup 127.0.0.1:/ 8.0E 402G 8.0E 1% /data overlay 30G 10G 18G 36% /etc/hosts /dev/xvdcz 30G 10G 18G 36% /etc/hostname shm 64M 0 64M 0% /dev/shm tmpfs 2.0G 12K 2.0G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 2.0G 0 2.0G 0% /proc/acpi tmpfs 2.0G 0 2.0G 0% /proc/scsi tmpfs 2.0G 0 2.0G 0% /sys/firmware ","path":"infra/aws/how-to-enable-efs-support-in-fargate-for-eks.html","title":"How to enable EFS support in Fargate for EKS"},{"content":"AWS has released fargate support for EKS for more than a year, I think it should be stable now and I'm investigating on how to use it as our serverless etl compute resources.before we moved fargate, we have to manage a set of AutoScaling Groups to manage the eks worker nodes, in our etl flow we have to modify ASG desired capacity to spin up new node to join cluster , use the compute resources, after the etl job, we have to terminate the ec2 worker node, in a perfect world, this works fine, but when anything goes wrong in the etl job or job failed, we have to make sure we terminate the new launched ec2 worker nodes, otherwise we will waste money on unneeded compute resources Fargate change the world completely, with fargate, I could just spin up Job/Pods from yaml file , Fargate will spin up new worker node with desired compute resources (including cpu and memory), when we finish, no mater job fail or not, Fargate will terminate the worker node when pod get removed, this will release us from the ASG management hell. further more, Fargate will make sure each pod is running on an isolated worker node which means there is no side effect between each pod, so it's easy to do isolate problem as well. although the Fargate price for the compute resources is higher than normal ec2 but for a longer term if we just use the compute resources for a short time period, it will save us lots of money With so many benefit, I can't wait to start to use it, firstly we have to create an IAM role with AWS predefined policy AmazonEKSFargatePodExecutionRolePolicy , at the moment the policy is really allow fargate worker node to communicate with ECR to pull docker image, but I think AWS will add more permission into the policy later. Secondly we need to create a Fargate profile, in the fargate profile we need to specify a namespace and some label matcher. for my example, I would like all the etl job running in namespace flow with label infrastructure=fargate to run on Fargate worker node. when this is in place, I could spin up pod to run on Fargate just like yaml below apiVersion: v1 kind: Pod metadata: name: efs-app namespace: flow labels: infrastructure: fargate spec: containers: - name: app image: centos command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;while true; do echo $(date -u) &gt;&gt; /data/out; sleep 5; done&quot;] if we remove label infrastructure=fargate, the pod will be scheduled on existing worker node, otherwise it will be scheduled on Fargate worker node and the worker node will be terminated as soon as Pod get deleted (really cool) then I continue to test some other features with Fargate, e.g: mount EFS into Fargate worker nodes, I have to deleted some existing fargate profile and create a new profile. everything seems working fine, but later on the day, some app developer contact me about they are not able to connect to our EKS (dev) env any more, when I check the cluster health, I noticed that all the normal ec2 worker nodes stuck in &quot;NotReady&quot; status, and if I describe any worker node , in the event I can see some error like below Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure Unknown Thu, 27 May 2021 11:51:41 +0800 Thu, 27 May 2021 11:55:52 +0800 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Thu, 27 May 2021 11:51:41 +0800 Thu, 27 May 2021 11:55:52 +0800 NodeStatusUnknown Kubelet stopped posting node status. PIDPressure Unknown Thu, 27 May 2021 11:51:41 +0800 Thu, 27 May 2021 11:55:52 +0800 NodeStatusUnknown Kubelet stopped posting node status. Ready Unknown Thu, 27 May 2021 11:51:41 +0800 Thu, 27 May 2021 11:55:52 +0800 NodeStatusUnknown Kubelet stopped posting node status. all the kubelet could not communicate with EKS api server any more so all the nodes are stuck in &quot;NotReady&quot; status. this doesn't make sense, we spend a day to investigate the issue, luckily one of our sr. engineer found out the root cause, in the cloud watch log, we have some api auth error to says that the Role are not mapped correctly in aws-auth configmap. then we come and check the value of aws-auth configmap in kube-system namespace and that's where cause the problem. our aws-auth configmap was over-written by some naughty aws process I believe at some point (but AWS may not admit it) . the fact is our aws-auth does not have the default EKS rolearn, instead it only contains the fargate profile role data: mapRoles: | - groups: - system:bootstrappers - system:nodes - system:node-proxier rolearn: arn:aws:iam::xxxxxxxx:role/acme-eks-fargate-pod username: system:node:{{SessionName}} while the expected aws-auth configmap should look like below data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxx:role/acme-eks-worker-nodes-NodeInstanceRole-xxxxxxx username: system:node:{{EC2PrivateDNSName}} - groups: - system:bootstrappers - system:nodes - system:node-proxier rolearn: arn:aws:iam::xxxxxxxx:role/acme-eks-fargate-pod username: system:node:{{SessionName}} The 1st part is over-written so all the normal worker nodes could could not obtain STS token to communicate with API server, then cause this issue, after we set the default rolearn back, everything come back to normal.","path":"infra/aws/a-journey-of-fixing-all-eks-worker-nodes-stuck-in-notready-status.html","title":"A journey of fixing all eks worker nodes stuck in NotReady status"},{"content":"I have a task to download 200M images from provider's image server as quick as possible, generally we use requests lib to do http communication, to speed up, I tried to use python's concurrency package to do the download in multiple thread, but Python's multiple thread is not real multiple thread, because if The Python Global Interpreter Lock(GIL). so it does not help that much when it's IO expensive operation. asyncio package really fits this requirement, it use an event loop to loop through different coroutine and speed things up, since python 3.4 it introduce 2 new key words async and await to work with asyncio package. here is a simple code sample to show the difference, to download 20 images, the async version takes 5seconds, but the sync version of code takes 31 seconds which is 6 times slower than async version. import asyncio import aiohttp import requests async def async_download_image(image): async with aiohttp.ClientSession() as session: async with session.get(f&quot;http://images.ipsensus.com:8080/{image}&quot;) as res: file = image.split(&quot;/&quot;)[-1] with open(f&quot;/tmp/async/{file}&quot;, &quot;wb&quot;) as f: print(f&quot;downloaded {file}&quot;) f.write(await res.read()) async def async_download_images(images): tasks = [asyncio.create_task(async_download_image(image)) for image in images] await asyncio.gather(*tasks) def sync_download_images(images): for image in images: r = requests.get(f&quot;http://images.ipsensus.com:8080/{image}&quot;) file = image.split(&quot;/&quot;)[-1] with open(f&quot;/tmp/sync/{file}&quot;, &quot;wb&quot;) as f: print(f&quot;downloaded {file}&quot;) f.write(r.content) if __name__ == \"__main__\": import time images = [ '003-007-001-12752/7f9695b3-6eaf-4d68-b2e3-8e00aafb7d73.png', '003-007-001-12753/899b781b-7584-4367-baf6-5a0224d70960.png', '003-007-001-12711/66553f65-f321-4a23-b747-9d7345717d35.png', '003-007-001-12711/b6399c73-f6b4-4911-a9d3-3fd17a5a8185.png', '003-007-001-12711/965db348-524a-4ac3-af7b-7048dde599d1.png', '003-007-001-12750/073e4d64-15c5-4f20-95f8-e7d29035277d.png', '003-007-001-12749/070e467b-c013-4e44-8e84-2b5645a803a3.png', '003-007-001-12749/9fc928d9-bd31-4fcf-b092-357392033eed.png', '003-007-001-12749/83570dbe-1bfa-4597-8500-0419cacfcd6b.png', '003-007-001-12751/20917d0e-d177-4b8e-ac57-407336cef4bf.png', '003-007-001-12747/bb7ed0ed-4783-40a2-87df-96338c4f9962.png', '003-007-001-12714/78b5126a-d374-4818-91db-acdc8e984c76.png', '003-007-001-12710/7307f5af-dab9-4cd8-bf91-94d39d394206.png', '003-007-001-12714/2e067bf9-ac31-4fff-9705-dd630bdd73a8.png', '003-007-001-12711/377d2fe3-cd2d-4c05-a06b-3c919f72682b.png', '003-007-001-12715/60de6045-2d0b-44a4-8a52-2281a6cdce4b.png', '003-007-001-12716/a798f5e0-4e46-4193-9c19-641c1e7d1858.png', '003-007-001-12759/ca3b678c-358c-4a68-a8dd-0b41d8cbc1ba.png', '003-007-001-12760/c8c4a7fd-01e6-4bb7-8d40-3e909a904777.png', '003-007-001-12760/d44dc080-6238-4c8e-991f-97126112c0d3.png' ] s = time.perf_counter() # asyncio.run(async_download_images(images)) sync_download_images(images) elapsed = time.perf_counter() - s print(f&quot;{__file__} executed in {elapsed:0.2f} seconds.&quot;) ","path":"programing/python/use-asyncio-to-accelerate-the-download-script.html","title":"Use Asyncio to accelerate the download script"},{"content":"We use Terraform to manage our infrastructures based on code as infrastructure approach, the code base is managed in git and multiple developers may change the same code base, one issue we got is someone change the infrastructure code, but forget to push to remote git repository or someone apply the terraform change but did not pull remote change first, this will end up with some unexpected result in the infrastructure. we develop a simple process to solve this issue the git check script a simple script to check if developer's local copy is in sync with git remote repos. #!/bin/bash # ensure that the terraform directory has no uncommitted changes if [[ $(git status --porcelain | grep 'terraform/') ]]; then echo &gt;&amp;2 &quot;Cannot Terraform with uncommitted changes under terraform/.&quot; exit 1 fi # ensure we are up-to-date with the upstream branch git remote update local_rev=$(git rev-parse @{0}) remote_rev=$(git rev-parse @{u}) if [[ $local_rev != $remote_rev ]]; then base_rev=$(git merge-base @{0} @{u}) if [[ $local_rev == $base_rev ]]; then echo &gt;&amp;2 &quot;Need to pull upstream changes before Terraform.&quot; elif [[ $remote_rev == $base_rev ]]; then echo &gt;&amp;2 &quot;Need to push changes upstream before Terraform.&quot; else echo &gt;&amp;2 &quot;Cannot Terraform: upstream branch has diverged.&quot; fi exit 1 fi use Makefile to execute all terraform goal we use Makefile to control every terraform execution, in the make file we could define a goal named &quot;git_check&quot; to call the script above git_check: ./git_check.sh for each execution goal we could make it depends on goal git_check, this will make sure all the local copy is in sync with remote repos before executing any terraform goal dns_apply: git_check cd network/route53/ &amp;&amp; \\ terraform apply plan.bin ","path":"infra/terraform-hook-to-make-sure-tf-file-is-in-sync-with-remote-git.html","title":"Terraform hook to make sure tf file is in sync with remote git"},{"content":" normal function function sayHello(firstName, lastName) { return `hello ${firstName} ${lastName}`; } function expression const sayHello2 = function(firstName, lastName) { return `hello ${firstName} ${lastName}`; }; arrow function const sayHello3 = (firstName, lastName) =&gt; `hello ${firstName} ${lastName}`; if you want to return an object in arrow function, we could do something like below const makeAToy = (color, size) =&gt; ({ toyColor: color, toySize: size }); Immediately Invoked Function Expression the function will be executed immediately (function(firstName, lastName) { console.info(`hello ${firstName} ${lastName}`); })('Amy', 'Li'); methods in object const greeting = { word: 'greeting', sayHello1(firstName, lastName) { console.log(`${this.word} ${firstName} ${lastName}`); }, sayHello2(firstName, lastName) { console.log(`${this.word} ${firstName} ${lastName}`); }, sayHello3: (firstName, lastName) =&gt; { console.log(`${greeting.word} ${firstName} ${lastName}`); }, }; call back function setTimeout(() =&gt; console.log('time to do something'), 1000); ","path":"programing/javascript/6-ways-to-declare-js-function.html","title":"6 ways to declare js function"},{"content":"we have a mission critical mysql database hosted on premise for many years, as the data growing it has reached more than 10T of data, it burn us a lot of effort to maintain the database, so we decide to migrate it into AWS RDS. As it's mission critical data, there is no way to take the database offline for migration, we must migrate the database while the data base still accept online transaction and no data lost allowed. this is very challengeable when the task land on my shoulder as I'm not a DBA, I tried to use AWS DMS to do the migration, but DMS is too slow for such large data base, I figure out another way to complete the task as below AWS RDS support extrabackup format natively, so I could use extrabackup to export the whole database physically and record the backup binlog position, then we could restore the xbackup into AWS, then I could make RDS a slave instance of current on premise mysql to syncup the data change since last backup. with this approach, the on premise database could continue to serve the on line transaction and we could do db switch after slave has synced with master. Step 1 mysql binlog setting the estimation of the whole migration will take about 10 days, so we must set the binlog size large enough to hold all the transactions in the binlog , so that we could catch up later step 2 attach backup disk attach backup disk to on premise database instance, the original disk is around 10T of data, so we must attach another 12T disk to store the xbackup data step 3 install extrabackup yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm yum list | grep percona yum install percona-xtrabackup-24 step 4 backup database into backup folder this will take about 75 hours to complete for 10T database, the command below will connect to local mysql db as user admin and export data into folder /mnt/data4/backup in tar format, it will split the file on every 500GB screen -R backup time xtrabackup --backup --user=admin --password=xxxxxxx --stream=tar --target-dir=/mnt/data4/backup | split -d --bytes=500GB - /mnt/data4/backup/masterdata.tar step 5 upload data to aws s3 after we backup the data , we need to upload the data into s3 so that aws RDS could read from the upload will take about 48 hours when happen in US east region screen -R backup cd /mnt/data4 time aws s3 cp --recursive backup s3://some_buckeet/backup step 6 create RDS based on xtrabackup from S3 go to aws console https://console.aws.amazon.com/rds/home?region=us-east-1#databases: click button &quot;Restore from S3&quot; s3 bucket: some_buckeet s3 prefex: backup Engine type: Amazon Aurora Version: Aurora(MySQL 5.6) 1.23.0 (as we have mysql 5.5 on premise, we could go beyond just 1 major version above, we could migrate to Aurora 2 afterwards) IAM role: aurora-restore-from-s3, we need an create a role , so that Aurora instance could read data from given s3 bucket ... the rest standard create database options the DB restore will take about 88 hours, after db restored successfully, RDS console will show current binlog position like below in event tab Recovered from Binary log filename 'mysql-bin.nnnnn', position 'mmmmmm'. this information is very important, we need to use this to setup the master slave replication , so that the RDS could catch up with on premise mysql database step 7 RDS catch up with on premise database after the database has restored from s3, it's time to catch up with on premise data to make the 2 databases identical need to allow public traffic for the RDS , so that it could communicate with on premise database need security group in place to only allow on premise database communicate with RDS connect to RDS to setup the slave like below -- set the binlog position CALL mysql.rds_set_external_master ('on premise mysql ip', 3306,'rdsrepl', 'xxxx', 'mysql-bin.nnnnn', mmmmmm, 0); -- start the replication CALL mysql.rds_start_replication; -- check slave status SHOW SLAVE STATUS \\G the cache up will take about 60 hours, there is a know bug from mysql 5.6 that , we need to update DB parameter group set query_cache_type to 0 to turn off the query cache, otherwise the replication will stop at “invalidating query cache entries”, see details from https://bugs.mysql.com/bug.php?id=60696 step 8 switch application endpoint to new RDS it's come to an excited point, that the 2 databases are identical now, we could to switch the application db url on standard maintenance window. during the standard maintenace window, we could do the following stop online applications stop db replications update db url to point to RDS start online application Now it's time to celebrate!","path":"data/datastore/migrate-10t-data-from-on-premise-to-rds-without-interuption.html","title":"Migrate 10T data from On Premise to RDS without interuption"},{"content":"Migrating database is never an easy task, I've recently migrated 29 databases from 5 different mysql instances into 3 AWS RDS mysql instances with the help of AWS Database Migration Service. I will record all the pain points I've met in this series. The first problem I met is that DMS only migrate data + primary key. for all the other database objects (view/trigger/store procedure/function) , DMS does not migrate them automatically, I have to migrate them manually. for an old database instance, it's hard to find the the up to date DDL/DML, the best way is to extract these database objects from live database. There is no way to like show create table statement to show how to create index, so I have to construct my own SQL to extract these type of information SELECT CONCAT('ALTER TABLE `', TABLE_NAME, '` ADD ', IF(NON_UNIQUE = 1, CASE UPPER(INDEX_TYPE) WHEN 'FULLTEXT' THEN 'FULLTEXT INDEX' WHEN 'SPATIAL' THEN 'SPATIAL INDEX' ELSE CONCAT('INDEX `',INDEX_NAME,'` USING ',INDEX_TYPE) END, CONCAT('UNIQUE INDEX `',INDEX_NAME,'` USING ',INDEX_TYPE) ), ' (',GROUP_CONCAT(DISTINCT CONCAT('`',COLUMN_NAME,'`') ORDER BY SEQ_IN_INDEX ASC SEPARATOR ','),')' ) as show_add_indexes FROM information_schema.STATISTICS WHERE TABLE_SCHEMA='&lt;database_name&gt;' AND INDEX_NAME &lt;&gt; 'PRIMARY' GROUP BY TABLE_NAME, INDEX_NAME ORDER BY TABLE_NAME ASC, INDEX_NAME ASC; To check if there is any view available for given database, we could use the following command to check SELECT TABLE_NAME,TABLE_TYPE,ENGINE,VERSION FROM information_schema.tables WHERE table_schema='&lt;database_name&gt;'; then we could use SHOW CREATE VIEW &lt;database_name&gt;.&lt;view_name&gt; \\G to extract the create view DDL To check if there is any trigger for given database, we could use the following command to check show triggers from &lt;database_name&gt; \\G then we could use SHOW CREATE TRIGGER &lt;database_name&gt;.&lt;trigger_name&gt; to extract the create trigger DDL To check store procedures and functions we could use SHOW PROCEDURE STATUS WHERE Db = '&lt;database_name&gt;' \\G SHOW FUNCTION STATUS WHERE Db = '&lt;database_name&gt;' \\G After we collect all the database object we could start the DB migration, it's recommeded to do full data load first, then pause and apply these DDL, then resume the CDC(ongoing replication) process until we swich all the application to target database.","path":"infra/aws/using-aws-dms-to-migrate-database-part-1.html","title":"Using AWS DMS to migrate database part 1"},{"content":"when we access a website over https, there is always a complex process happen under the hood between your browser and remote websrever to ensure the communicate is secure. there are some concepts involved before we could explian the whole process asymmetric cryptography symmetric cryptography certificate authority(CA) Client Hello: browser will send out a random number random1 and a list of encryption algorithms algos it could understand to server over http. different kind of browsers with different versions could support different encryption algorithms. Server Hello: server will send out a random number random2 and selected encryption algorithm algo based on algos to client over http, the server has determined the client and server will communicate based on encryption algorithm algo going forward. Server Certificate: server will send out it's certificate to client over http, the certificate could be verified by CA to prove it comes from the server. Server Hello Done: server will send the server hello done msg to client over http to indicate it has no more info Client Verification: client will use the CA in it's OS to verify server's certificate and extract the public key from the certificate for further usage Gen pre-master: client will generate a pre-master Client Key Exchange: client could use server's public key to encrypt pre-master and send to server, server could use it's private key to decrypt pre-master Gen secret key: now both client and server know random1(from client),random2(from server) , pre-master(from client) and algo(from server), they could generate the symmetric secret key based on the information. Change Cipher Spec: the client sends the ChangeCipherSpec notification to the server and start the symmetric encryption handshake Encrypted Handshake Message: client will send handshake message to server based on algo and the generated secret key Change Cipher Spec: the server sends the ChangeCipherSpec notification to the client and start the symmetric encryption handshake Encrypted Handshake Message: server will send handshake message to client based on algo and the generated secret key Finish: going forward the client and server could both understand the encrypted message based on algo and the generated secret key so they could communicated over encrypted channel from then on ","path":"infra/what-happen-when-you-access-webstie-over-https.html","title":"What Happen when you access webstie over https"},{"content":"access.log is very useful for nginx , but it's hard analyse it , today I find GoAccess, it's written in C and very efficient. I have a CentOS 7 box running nginx, the log is stored in /var/log/nginx/access.log, it also config to listen on port 80 for folder /usr/share/nginx/html/, so with a simple command below goaccess --agent-list --output=/usr/share/nginx/html/goaccess.html --log-format COMBINED --real-time-html /var/log/nginx/access.log, we could see the real time access log analyse result from browser like below it also have pre-built images from https://hub.docker.com/r/allinurl/goaccess/, I will look into how to integrate it into ingress-nginx next.","path":"infra/use-goaccess-to-analyse-nginx-accesslog.html","title":"Use goaccess to analyse nginx accesslog"},{"content":"Kong has lots of plugins available to do varies kind of tasks, Security is a big topic among the plugins , no one wants to expose API to internet without any protection. we use HMAC Plugin to do API level authentication. with HMAC Plugin the password never go through the wire, and we could also enable the body verification as an option. this is a good option for us to open API to the public internet enable the hmac auth plugin apiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: hmac-auth plugin: hmac-auth this will enable the plugin in k8s env and we could enable the plugin by putting an annotation value in the ingress yaml file annotations: plugins.konghq.com: hmac-auth config username and password hmac plugin need a Consumer and 1 or more credentials, the credentials should be attached to the Consumer apiVersion: configuration.konghq.com/v1 kind: KongConsumer metadata: name: consumer-hmac username: api-user --- apiVersion: configuration.konghq.com/v1 kind: KongCredential metadata: name: credential-hmac consumerRef: consumer-hmac type: hmac-auth config: username: whoami1 secret: xxxxxxxxxxxxx --- apiVersion: configuration.konghq.com/v1 kind: KongCredential metadata: name: credential-hmac consumerRef: consumer-hmac type: hmac-auth config: username: whoami2 secret: xxxxxxxxxxxxx when we deploy these consumere and credentails, the ingress which enalbe the hmac-auth plugin is protected. generate client code to call the server to call the hmac-auth protected api, we must generate the client signature and pass it in http header, to generate hmac hash, kong support sha1, sha256,sha384 and sha512, sha1 is not recommended as it's not secure, I will use sha256 in this article. like code below we could use password xxxxxxxxxxxxx to generate hash data for any given data private String hmacHash(String key, String data) throws Exception { String algorithm = &quot;HmacSHA256&quot;; SecretKeySpec signingKey = new SecretKeySpec(key.getBytes(), algorithm); Mac mac = Mac.getInstance(algorithm); mac.init(signingKey); return Base64.getEncoder().encodeToString(mac.doFinal(data.getBytes())); } with Kong hmac we must pass Date in GMT format through http header and pass Authorization for the calulcated hash, Authorization is formated as hmac username=&quot;whoami1&quot;, algorithm=&quot;hmac-sha256&quot;, headers=&quot;Date&quot;, signature=&quot;hmacHash(dateString, xxxxxxxxxxxxx)&quot; hmac is telling the server we are using hmac plugin to to authentication username=&quot;whoami1&quot; is the user name we created in the KongCredential, then server could located this user's password on server side algorithm=&quot;hmac-sha256&quot; will tell server we are using sha265 algorithm to calculate hash headers=&quot;Date&quot; will tell server we use Date as the input source to calculate hash signature=&quot;hmacHash(dateString, xxxxxxxxxxxxx)&quot; is the signature with result of hmacHash calculated on client side based on &quot;Date&quot; header to send back to server. the server will sueee the same password for whoami1 to caluclate hash for &quot;Date&quot; header, if the calculated hash match given signature, auth pass, otherwise auth failed. private Map&lt;String, String&gt; headers(String username, String password) throws Exception { Map&lt;String, String&gt; headers = Maps.newLinkedHashMap(); DateFormat df = new SimpleDateFormat(&quot;EEE, dd MMM yyyy HH:mm:ss&quot;); df.setTimeZone(TimeZone.getTimeZone(&quot;GMT&quot;)); String date = df.format(new Date()) + &quot; GMT&quot;; headers.put(&quot;Date&quot;, date); String rawSign = &quot;Date: &quot; + date; String authHeader = String.format( &quot;hmac username=\\&quot;%s\\&quot;, algorithm=\\&quot;hmac-sha256\\&quot;, headers=\\&quot;Date\\&quot;, signature=\\&quot;%s\\&quot;&quot;, username, hmacHash(password, rawSign)); headers.put(&quot;Authorization&quot;, authHeader); return headers; } test auth in java code code below will function well with only Google Guava as a dependency, we could use http client to write more elegant code to handle http request and response. @Test public void testRequest() throws Exception { Map&lt;String, String&gt; headers = headers(&quot;whoami1&quot;, &quot;xxxxxxxxxxxxx&quot;); URL url = new URL(&quot;http://api.vipmind.me/hello/v1&quot;); HttpURLConnection con = (HttpURLConnection) url.openConnection(); con.setRequestMethod(&quot;POST&quot;); con.setDoOutput(true); //set the header headers.entrySet().forEach(entry -&gt; con.setRequestProperty(entry.getKey(), entry.getValue())); //set the timeout con.setConnectTimeout(5000); con.setReadTimeout(5000); //send out the call the server try (OutputStream os = con.getOutputStream()) { byte[] input = Files.toByteArray(new File(Resources.getResource(&quot;data.json&quot;).getFile())); os.write(input, 0, input.length); } if (con.getResponseCode() == 200) { //receive response try (BufferedReader br = new BufferedReader( new InputStreamReader(con.getInputStream(), &quot;utf-8&quot;))) { StringBuilder response = new StringBuilder(); String responseLine = null; while ((responseLine = br.readLine()) != null) { response.append(responseLine.trim()); } System.out.println(response.toString()); } } else { try (BufferedReader br = new BufferedReader( new InputStreamReader(con.getErrorStream(), &quot;utf-8&quot;))) { StringBuilder response = new StringBuilder(); String responseLine = null; while ((responseLine = br.readLine()) != null) { response.append(responseLine.trim()); } System.out.println(response.toString()); } } con.disconnect(); } test auth in curl we could also construct curl command to call the api @Test public void testCurl() throws Exception { StringBuilder call = new StringBuilder(); call.append(&quot;curl -XPOST&quot;); Map&lt;String, String&gt; headers = headers(&quot;whoami1&quot;, &quot;xxxxxxxxxxxxx&quot;); headers.entrySet().forEach(entry -&gt; call .append(&quot; -H '&quot;) .append(entry.getKey()) .append(&quot;: &quot;) .append(entry.getValue()) .append(&quot;'&quot;)); call.append(&quot; -d \\&quot;@data.json\\&quot;&quot;); call.append(&quot; http://api.vipmind.me/hello/v1&quot;); System.out.println(call.toString()); } ","path":"infra/kube/how-to-use-hmac-plugin-to-protect-kong-api.html","title":"How to use HMAC Plugin to protect Kong API"},{"content":"In previous articles, I've introduced how to deploy airflow into kubernetes and how to organize airflow project in an efficinet way, in real development work, I've also seem someone use airflow to do big data processing. I think this is a very big missunderstaing about ariflow. in airflow's offical webiste, it already declared very clearly that Airflow is a platform to programmatically author, schedule and monitor workflows. but not declare itself as a data processing engine. in my view airflow is lacking of 2 critical data processing engine features. Airflow is lack of capability to pass data between tasks In a real world data processing flow job, it always involve mutiple steps to process the data, this include load data from external data source (e.g: database, csv files, SQS topics or Big Query Tables) pass the loaded data into next steps for processing , the process could include mutiple steps transformation or filtering persist result back to exterenal data sink Data processing engine, like Flink or Spark they support such concept out of box, to load and sink data , they have different connecoters to connect to different data source. after data is loaded into the engine, the data do not need to perisist into any intermediat storage to share between tasks, the intermediat data could be passed between different tasks out of box. for Flink it is using Kryo by default. but in airflow it does not support this, when we want to share data between tasks we have to persist the interemediate result to somewhere (e.g: database), then load the data in next tasks, this makes the data processing very insufficient . Airflow is a shared instance or cluster to manage data jobs Airflow is heavily using mysql and rabbitmq to manage the dags's task dependnecies and dag runs. Airflow actually is a collection of data jobs, this is ok when we just triggering / scheduling / monitoring data jobs, because in airflow we could set retries parameter to retry on failure, this may happen when we need to redeploy airflow to enalbe more jobs etc. but when we use airflow to as data processing engine, the data processing maybe interruppted and airflow has no good way to resume the interruppted job without data lost. when we get more and more data processing jobs in airflow , it will be more and more offten that the data processing job will be impacted by other jobs. on the ohter hand , data processing engine like Flink or Spark could be deployed as single data process job per clusetr approach, e.g: we could depliy Spark job into kubernetes for just one job. this architechture could guarantee that each data processing job is running in an isolated env, so no cross job impact.this gives the developer a great freedom to develop / manage run their own data job without need to think of impact others. So if you plan to use airflow, calm down for a second,think about what you really want from airflow, if you want the ability to triggering / scheduling / monitoring, go for it , it will do a great job in this area, if you want to process big data, no, no ,no, think about Flink or Spark instead.","path":"infra/kube/airflow-is-not-a-data-processing-engine.html","title":"Airflow is not a data processing engine"},{"content":"When we have more and more team members joining the development team, we may want make sure the code quarlity does not drop down and make the code style consistent, in Python world there are tools exist to make sure this happen. pytest-cov pytest-cov is a pytest plugin to help us to calculate the code coverage and enforce code coverage to be at some level, to install the package use pipenv install pytest-cov --dev to make it as a development dependency, to config it add the following lines into tox.ini, it will work, we could modify --cov=xx to include the package we want to enforce the coverage, in grpc-mate we could simple ignore the auto generated code [tool:pytest] [pytest] addopts = --cov=service/ --cov=data_store/ --cov-fail-under=90 after we run make pytest we could get result like below, if the coverage does not meet expectation, the build will fail ---------- coverage: platform darwin, python 3.6.8-final-0 ----------- Name Stmts Miss Cover -------------------------------------------------------- data_store/__init__.py 6 0 100% data_store/db.py 11 3 73% data_store/models.py 10 0 100% service/__init__.py 0 0 100% service/greeter_servicer.py 8 0 100% service/product_read_servicer.py 36 2 94% service/product_update_servicer.py 16 0 100% -------------------------------------------------------- TOTAL 87 5 94% Required test coverage of 90% reached. Total coverage: 94.25% pycodestyle PEP 8 is the standard python style guide line, and pycodestyle is the tool to make sure our code is align with PEP 8, to install the package use pipenv install pycodestyle --dev to make it as a development dependency, to config it , add the following lines into tox.ini, it will work, in the configuration we could also config which type of check we could ignore and override some default PEP 8 config [pycodestyle] ignore = E722 max-line-length = 120 statistics = True if anything does not align with the PEP 8, the build will fail. After we config these tools we could make sure all the developers share the same code style and make sure the code coverage never drop down, we also need CI tool's support to build the project automaticaly, fail the build when any config rule gets broken and send out alert to responsible developers to fix.","path":"programing/python/how-to-enforce-code-coverage-and-code-style-in-python-project.html","title":"How to enforce Code Coverage and Code Style in Python Project"},{"content":"MapReduce is an old technology in nowadays, but its concept is still valid in data processing world, in previous article I've talked about the Map part, today I'm going to talk about the Reduce part in Flink. In Flink Streaming Application we could put data into window, after we put the data into window, the data will have a boundary, we could calcualte aggregate result based on the limited data in the window Reduce Function Reduce Function combine groups of elements to a single value, by taking always two elements and combining them into one. for example we could use Reduce Function to calcualte the min/max value in the window. the code below shows that we first group the students into group by their year, then calcualte the min score of the year within the window object ReduceFn { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val stream = env.fromElements( Student(&quot;Amy&quot;, 2, 100), Student(&quot;Emily&quot;, 3, 60), Student(&quot;Kelly&quot;, 2, 80), Student(&quot;Mia&quot;, 3, 70), Student(&quot;Selina&quot;, 2, 75)) .assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[Student] { override def getCurrentWatermark: Watermark = new Watermark(System.currentTimeMillis()) override def extractTimestamp(t: Student, l: Long): Long = System.currentTimeMillis() }) .keyBy(_.year) .timeWindow(Time.seconds(1)) .reduce((s1, s2) =&gt; { if (s1.score &gt; s2.score) { s2 } else { s1 } }) stream.print() env.execute() } } Aggregate Function Aggregate Function has more flexibility than Reduce function, it could have an intermediate aggregate state called accumulator, Values are added to the accumulator, and final aggregates are obtained by finalizing the accumulator state. This supports aggregation functions where the intermediate state needs to be different than the aggregated values and the final result type the code below shows that we first group the students into group by their year, then calcualte the average score of the year within the window, the Accumulator could hold the intermediate values for the whole window before getResult is called object AggregateFn { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val stream = env.fromElements( Student(&quot;Amy&quot;, 2, 100), Student(&quot;Emily&quot;, 3, 60), Student(&quot;Kelly&quot;, 2, 80), Student(&quot;Mia&quot;, 3, 70), Student(&quot;Selina&quot;, 2, 75)) .assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[Student] { override def getCurrentWatermark: Watermark = new Watermark(System.currentTimeMillis()) override def extractTimestamp(t: Student, l: Long): Long = System.currentTimeMillis() }) .keyBy(_.year) .timeWindow(Time.seconds(1)) .aggregate(new AvgScoreFunction()) stream.print() env.execute() } } class AvgScoreFunction extends AggregateFunction[Student, (Int, Int, Int), (Int, Double)] { override def createAccumulator(): (Int, Int, Int) = (0, 0, 0) //(year,studentCount, totalScore) override def add(in: Student, acc: (Int, Int, Int)): (Int, Int, Int) = (in.year, acc._2 + 1, acc._3 + in.score) override def getResult(acc: (Int, Int, Int)): (Int, Double) = (acc._1, acc._3 / acc._2) override def merge(acc1: (Int, Int, Int), acc2: (Int, Int, Int)): (Int, Int, Int) = (acc1._1, acc1._2 + acc2._2, acc1._3 + acc2._3) } ProcessWindowFunction ProcessWindowFunction has the most flexibility , but it is also the most complex reduce/aggregate function to implement, in process method, it could access an iterable type of input elements and do any logic you want by iterating the elements and use Collector to collect any amout of output you want. compare with ReduceFunction and AggregateFunction's state, ProcessWindowFunction need more space to hold the intermediate state because it need to store all the input elements. so we may need to be careful when using ProcessWindowFunction. the code below shows that we first group the students into group by their year, then calcualte both the max score and the min score of the year within the window object ProcessWindowFn { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val stream = env.fromElements( Student(&quot;Amy&quot;, 2, 100), Student(&quot;Emily&quot;, 3, 60), Student(&quot;Kelly&quot;, 2, 80), Student(&quot;Mia&quot;, 3, 70), Student(&quot;Selina&quot;, 2, 75)) .assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[Student] { override def getCurrentWatermark: Watermark = new Watermark(System.currentTimeMillis()) override def extractTimestamp(t: Student, l: Long): Long = System.currentTimeMillis() }) .keyBy(_.year) .timeWindow(Time.seconds(1)) .process(new MinMaxScoreOfYearFunction()) stream.print() env.execute() } } case class MinMaxScoreOfYear(year: Int, min: Int, max: Int, endTime: Long) class MinMaxScoreOfYearFunction extends ProcessWindowFunction[Student, MinMaxScoreOfYear, Int, TimeWindow] { override def process(key: Int, context: Context, elements: Iterable[Student], out: Collector[MinMaxScoreOfYear]): Unit = { val scores = elements.map(_.score) val windowEnd = context.window.getEnd out.collect(MinMaxScoreOfYear(key, scores.min, scores.max, windowEnd)) } } ","path":"data/flink/flink-windowfunctions.html","title":"Flink WindowFunctions "},{"content":"It has been about a year since I last configed Kong Ingress Controller in kubernetes, yesterday when I was trying to config Kong Ingress controller to expose an API on kubernetes, I met a new concept KongIngress. Kong is an API gateway platform , it start to support kubernetes since May 2018,it's a great platform to manage APIs, Kong has a plugin system to offer large amount of capability to manage open APIs, e.g: authentication, load balancing, traffic control, analytics, logging etc. with Kong , the developer could focus on business logic development and let Kong to do all the generic api management work. Kong Ingress controllere could be deployed into multiple platforms, e.g EKS, GKE, AKS etc when it's deployed to EKS , Kong Ingress controller will create a ELB it process all the request on layer4, the ELB could handle both http and https , the port will map to NodePort in kubernetes ec2 instance apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;kong-proxy&quot;,&quot;namespace&quot;:&quot;kong&quot;},&quot;spec&quot;:{&quot;ports&quot;:[{&quot;name&quot;:&quot;kong-proxy&quot;,&quot;port&quot;:80,&quot;protocol&quot;:&quot;TCP&quot;,&quot;targetPort&quot;:8000},{&quot;name&quot;:&quot;kong-proxy-ssl&quot;,&quot;port&quot;:443,&quot;protocol&quot;:&quot;TCP&quot;,&quot;targetPort&quot;:8443}],&quot;selector&quot;:{&quot;app&quot;:&quot;kong&quot;},&quot;type&quot;:&quot;LoadBalancer&quot;}} name: kong-proxy namespace: kong selfLink: /api/v1/namespaces/kong/services/kong-proxy spec: clusterIP: xxx.xxx.xxx.xxx externalTrafficPolicy: Cluster ports: - name: kong-proxy nodePort: 30232 port: 80 protocol: TCP targetPort: 8000 - name: kong-proxy-ssl nodePort: 32021 port: 443 protocol: TCP targetPort: 8443 selector: app: kong sessionAffinity: None type: LoadBalancer status: loadBalancer: ingress: - hostname: xxxxx.xxxx.elb.amazonaws.com each Kong Ingress controller will monitor the Ingress resources in all the namespaces in the EKS clustter and create route rull based on Ingress resources, Kong Ingress controller could live other ingress controller as well, we need to speicfy kubernetes.io/ingress.class: &quot;kong&quot; in Ingress annotation to tell Kong to react on this Ingress creation previously the Ingress below will work, it will route all the request match path /api to service my-api on port 9000 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: my-api namespace: api annotations: kubernetes.io/ingress.class: &quot;kong&quot; spec: rules: - host: api.vipmind.me http: paths: - path: /api backend: serviceName: my-api servicePort: 9000 when we upgrade to Kong 1.2, it stops working, all the path we got in service my-api is just /, here comes KongIngress to resolve the issue, we need to first create a KongIngress resources like below apiVersion: configuration.konghq.com/v1 kind: KongIngress metadata: name: api-kong-ingress namespace: api route: strip_path: false then patch configuration.konghq.com: &quot;api-kong-ingress&quot; as Ingress annotation to avoid Kong to strip the path. KongIngress is acting as an extenion of Ingress resources,it could also be used as a way to remap url for banckend services, please see KongIngress custom resources for details.","path":"infra/kube/kongingress-in-kong.html","title":"KongIngress in Kong"},{"content":"Tyler Akidau published Streaming 101: The world beyond batch up on Oreilly in 2015, this is the fundamental theory of Google Cloud DataFlow, nowadays, the API of Google Cloud DataFlow has become Apache Beam, Apache Flink has the same concept of how Apache Beam and Google Cloud DataFlow processing streaming applications. I'm going to talk about how to use 2 critical concepts Time and Window in Flink here. for detail theory, please refer Streaming 101: The world beyond batch for details. Event Time VS Processing Time Event time: which is the time at which events actually occurred. Processing time: which is the time at which events are observed in the system. Some Application may not care about the difference between event time and processing time, this type of application is easier to handle. but a lot of applicants need to handle this, think about an application that handle a game player's rank in real time, on player is playing the game , he will generate a serious of event and push to the data procesing system, in a perfect world, the event will be pushed into the data processing system seamlessly, then the Event Time and Processing time has the same trend, but in real world , it could be differnet, think about the player is playing the game on a train, the train is passing a tunnel, there is no 4G signal in the tunnel, so the event will be buffered in the player's device , after the train pass the tunnel, the device will send out all the buffered event to the data processing system, in this case the event will be process late. when we calculate result based on the event time, this player's score will be wrong when he is in the tunnel. Flink support both Event Time Stream and Processing Time Stream, we could set the application's characteristic, like below val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) //or env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) Window A stream application is an endless data stream, if you want to do any statistics or aggregation, you will have to do the calculation based on a boundary, e.g: what's the average score every 5 mins or what's the max score every 5 mins, Window is designed to support such feature, Flink support both key based window and none-key based window, the key based window will be evaluated in parallel , none-key based window will be evaluate in a single thread. for key based window, Flink support 3 types of windows for both Event Time and Processing Time. TumblingWindows The event will be placed into a timed window side by side, we could define the window interval and the framework will assign the event into the right window, there is no overlap between windows, every event will be assing to one window for processing, example below assign event to window every 1 minute and it will trigger the processing function when all the event time or processing time has passed the window stream.keyBy(_.year) .timeWindow(Time.minutes(1)) SlidingWindows a sliding window will define a window size and an interval SlidingEventTimeWindows.of(Time.hours(1),Time.minutes(15)) will define a 1hour time window every event within this hour will be placed into sliding window 4 times as time passed SessionWindows an event in a session window will be placed into a window, the session window will define a gap time, the window will be triggered when no event arrived exceed the defined gap. EventTimeSessionWindows.withGap(Time.minutes(1)) will trigger the session window if no more event with given key more than 1 minute","path":"data/flink/time-and-window-in-flink-stream-application.html","title":"Time and Window in Flink Stream Application"},{"content":"In a stream application, it's very common to split the stream into multiple streams and apply different logic, in Flink there are 2 ways to do that SideOutput Stream This is the preferred way to split Flink stream into multiple streams, the advantange obout SideOutput is that the SideOutput do not need to be the same type as the main stream, and the main stream is not impacted on the side output stream like example below, we have a stream of Student mix of year 2 and year3, we could apply a ProcessFunction to the main stream and split the main stream into 2 sub streams with different tags &quot;year2 students&quot; and &quot;year3 students&quot;, the side output stream's type is String which is different from main stream type Student , the main stream could continue process without any side effect from the side output streams. object SideOutputStream { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val stream = env.fromElements( Student(&quot;Amy&quot;, 2, 100), Student(&quot;Emily&quot;, 3, 60), Student(&quot;Kelly&quot;, 2, 80), Student(&quot;Mia&quot;, 3, 70), Student(&quot;Selina&quot;, 2, 75)) .process(new StudentYearProcessor()) val year2StudentStream = stream.getSideOutput(new OutputTag[String](&quot;year2 students&quot;)) .map(_.toLowerCase) val year3StudentStream = stream.getSideOutput(new OutputTag[String](&quot;year3 students&quot;)) .map(_.toUpperCase) year2StudentStream.print() year3StudentStream.print() stream.print() env.execute() } } class StudentYearProcessor extends ProcessFunction[Student, Student] { lazy val year2StudentOutput: OutputTag[String] = new OutputTag[String](&quot;year2 students&quot;) lazy val year3StudentOutput: OutputTag[String] = new OutputTag[String](&quot;year3 students&quot;) override def processElement(student: Student, context: ProcessFunction[Student, Student]#Context, collector: Collector[Student]): Unit = { if (student.year == 2) { context.output(year2StudentOutput, student.name) } if (student.year == 3) { context.output(year3StudentOutput, student.name) } collector.collect(student) } } SplitStream the other way to achive the same thing is to use split method on DataStream API, we will get a SplitStream , this method is already deprecated since Flink 1.6 , compare with Side OutputStream , SplitStream must use the same type of main stream and after the main stream get splitted, the main stream is gone. see example below object SplitStream { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val splitStream = env.fromElements( Student(&quot;Amy&quot;, 2, 100), Student(&quot;Emily&quot;, 3, 60), Student(&quot;Kelly&quot;, 2, 80), Student(&quot;Mia&quot;, 3, 70), Student(&quot;Selina&quot;, 2, 75)) //split is deprecated since flink 1.6 use side out put instead .split(s =&gt; s.year match { case 2 =&gt; Seq(&quot;y2&quot;) case 3 =&gt; Seq(&quot;y3&quot;) }) val y2Stream = splitStream.select(&quot;y2&quot;) .map(s =&gt; Student(s.name.toLowerCase, s.year, s.score)).setParallelism(2) val y3Stream = splitStream.select(&quot;y3&quot;) .map(s =&gt; Student(s.name.toUpperCase, s.year, s.score)).setParallelism(3) y2Stream.print().setParallelism(2) y3Stream.print().setParallelism(1) env.execute(&quot;Split stream&quot;) } } ","path":"data/flink/flink-stream-branches.html","title":"Flink Stream Branches"},{"content":"Apache Beam could be used as API layer of Apache Flink, function is the function is the fundamental ops in these 2 frameworks, a stream of data of a bundle of data would be send to functions for processing in parallel across many different nodes. in Beam it use at-least-once strategy to process data in functions which means if a function failed for given stream or bundle of data, these stream or bundle of data will be send to the same function in other nodes to retry, in flink, it provide exactly-once support with it's state management which means flink will guaranttee that any data in the steam of bundle of data would only be processed once this is a very hard goal to achieve. in this article, I will talk about how use function in both beam and flink to process data. flink lambda functions most of flink api support lambda function in both scala and java version, if there is no complex logic in the function, it's really neat to use lambda function , like below, assume we have a case class named Student case class Student(name: String, year: Int, score: Int) object BasicTransform { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.fromElements( Student(&quot;Amy&quot;, 2, 100), Student(&quot;Emily&quot;, 3, 60), Student(&quot;Kelly&quot;, 2, 80), Student(&quot;Mia&quot;, 3, 70), Student(&quot;Selina&quot;, 2, 75)) .map(s =&gt; Student(s.name.toLowerCase, s.year, s.score)) .filter(s =&gt; s.score &gt; 70) .flatMap(s =&gt; List(s, s)) .print() env.execute(&quot;basic stream&quot;) } } in the code above, we will map the student's name to lower case and filter out those student whoes score is less than 70 and also double the output by flapmap function. flink rich functions every flink lambda function will have a related rich functions, the difference between lambda functions and rich functions is that rich functions has open and close methods to override , the open method is called before the actual working methods (like map or join) and thus suitable for one time setup work. For functions that are part of an iteration, this method will be invoked at the beginning of each iteration superstep. the close method called after the last call to the main working methods (e.g. map or join). For functions that are part of an iteration, this method will be invoked after each iteration superstep. The code below is showing how to use RichFlatMap function, it could have a private attribute named subTaskIndex, it could be initialized in open method and tear down in close method. the function class will be serialized and send across different worker nodes in runtime, so it's critical to make sure the function class is serializable, and makr the private field as transient when needed to avoid serialization effort. object RichFlatMapFn { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val stream = env.fromElements( Student(&quot;Amy&quot;, 2, 100), Student(&quot;Emily&quot;, 3, 60), Student(&quot;Kelly&quot;, 2, 80), Student(&quot;Mia&quot;, 3, 70), Student(&quot;Selina&quot;, 2, 75)) .flatMap(new StudentFlatMapFn) stream.print() env.execute() } } class StudentFlatMapFn extends RichFlatMapFunction[Student, (Int, Student)] { @transient var subTaskIndex = 0 override def open(config: Configuration): Unit = subTaskIndex = getRuntimeContext.getIndexOfThisSubtask override def close(): Unit = super.close() override def flatMap(in: Student, collector: Collector[(Int, Student)]): Unit = { for (idx &lt;- 0 to subTaskIndex) { collector.collect(idx, in) } } } beam functions Beam functions is more like Flink rich functions, a basic Beam function will be like below, the method which is annotate by @DoFn.Setup will be called before any DoFn which is similar to Flink's open method and the method which is annotate by @DoFn.Teardown will be called to clean up this instance before it is discarded. No other method will be called after a call to the annotated method is made which is similar to Flink's close method import org.apache.beam.sdk.transforms.DoFn; import org.slf4j.Logger; import org.slf4j.LoggerFactory; /** * @author ivan */ public class XXFn&lt;I, O&gt; extends DoFn&lt;I, O&gt; { private static Logger log = LoggerFactory.getLogger(XXFn.class); //NOPMD @Setup public final void setup() throws Exception { log.debug(&quot;Setup called&quot;); } @Teardown public final void teardown() throws Exception { log.info(&quot;Teardown complete&quot;); } @ProcessElement public final void processElement(final ProcessContext context) throws Exception { } } Beam also provide annotation like @DoFn.StartBundle and @DoFn.FinishBundle to be called before and after a bundle of data has been processed , on the other hand Flink also provide ways for some other fine control, e.g: we could setup Flink's parallelism for each task separately , but Beam API does not provide such capacity to do so.","path":"data/flink/beam-and-flink-functions.html","title":"Beam and Flink Functions"},{"content":"I've been using Apache Beam for many years to process big data, Apache Beam support lots of runtime under the hood, e.g Apache Flink, Apache Spark, Google Cloud Dataflow, I'm using Apache Flink as beam runtime , I'm also using kubernetes to create and manage flink cluster in kubernetes cluster ,this is running very well, but from Beam to Flink there is a translation phase to translate Beam API to Flink executable code, the overhead is minimum, but I'm always curious about how to do it directly via Flink API directly. the other reason why I want to try Flink directly is that Beam support Java/Go/Python but it does not support Scala out of box, on the other hand Flink support both Java and Scala out of box, I also want to play with Scala, this is why the Flinker project is created. Flink have a template to bootstrap sample flink project out of box, we could use maven archetype to generate it easily. mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-scala \\ -DarchetypeVersion=1.9.1 \\ -DgroupId=me.vipmind.flinker \\ -DartifactId=flinker-scala \\ -Dversion=1.0-SNAPSHOT \\ -Dpackage=me.vipmind.flinker \\ -DinteractiveMode=false This command will generate a project named flinker-scala it will use maven to build the project, by default the archetype plugin will generate flink project based on scala 2.11, I'm more prefer to use scala 2.12, so I change could just change the version in the pom.xml from 2.11 to 2.12 with this commit. to import the scala project into IntelliJ Idea, we could import the maven project as normal, but we need to enable maven profile add-dependencies-for-IDEA, so that IDEA could find Flink dependencies when comiple code. Flink also could generate java based project by the command below for flinker-java project mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-java \\ -DarchetypeVersion=1.9.1 \\ -DgroupId=me.vipmind.flinker \\ -DartifactId=flinker-java \\ -Dversion=1.0-SNAPSHOT \\ -Dpackage=me.vipmind.flinker \\ -DinteractiveMode=false I'm going to use flinker-scala as a base project to evaluate all the Flink features via Scala API in the following months.","path":"data/flink/why-i-make-flinker-project-and-how-to-bootstrap-a-flink-project.html","title":"Why I make Flinker project and how to bootstrap a Flink project"},{"content":"Recently I've developped some statistics logic to query mysql database and collect the results, every single query is optimized. but because of the logic, I will need to issue a lot of queries concurrently, I may need to issue about 353820000 sql queries agains mysql database within 24 hours. then the database is freeup. to be able to handle all the sql quries, I will need very large rds instnace e.g db.r5.4xlarge, but the problem is I will only need this about 24hours each week, it's IDLE the rest of the week. so using reserved rds instance will waste a lot of money, then I discovered aurora serverless Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. this is what I need, but keep in mind that there is No Silver Bullet, Aurora Serverless has it's own limitation, by the time of writing this article, it only support mysql version 5.6.10a . we are using mysql v5.7 in production this means I could not create read replica from master node directly, I will have to have a script to export the data and import it into Aurora Serverless. I use mysqldump to export the table I need for the queries , I just select a sub tables to export and ignore unrelated tables to save the export/import time. #export mysqldump -h prod-mysql-biz-master.vipmind.me -u xxx -pxxxxxx \\ --opt \\ --set-gtid-purged=OFF \\ --skip-triggers --skip-comments \\ --ignore-table=xxxxdb.xxx_table1 \\ --ignore-table=xxxxdb.xxx_table2 \\ --single-transaction \\ --databases xxxxdb &gt; xxxxdb.sql #import mysql -h prod-mysql-biz-stats.vipmind.me -u xxxx -pxxx &lt; xxxxdb.sql After the data is loaded into Aurora Serverless, we could config how to scale Aurora Serverless, it has a concept called Aurora Capacity Unit (ACU) to represent how much capacity will be avaialble in the db server, Aurora Serverless will do autoscaling based on the server load, if there is no connection/activities for a configged time period, the db capacity will be scalled down to 0 which means no cost at all for db server compute resources, but we do need to pay the db storage resources. Since I'm using HikariCP to manage the database conneciton pool, I will need to make sure the db connection pool will not hold any connection if there is no activity , so that Aurora Serverless could scale down to 0 ACU, within the HikariCP it's easy to do that, code below could do what I want when config HikariCP // do not use connections unless they are needed hikariConfig.setMinimumIdle(0); // maximum wait time for a connection to lay idle in pool - 5 minutes hikariConfig.setIdleTimeout(30000); // maximum wait time for a connection from the pool - 5 minutes hikariConfig.setConnectionTimeout(300000); The end result of this change works very well, I could complete the data flow job within 24 hours with Aurora Serverless scalue up to 64 ACU, the peak ACU I've seen is 128 ACU but it's just for a short period of time.after the flow job complete Aurora Serverless just scale down to 0 ACU it's really help me to save a lot of cost and also achieve very high analytics performance. ","path":"data/datastore/use-aws-aurora-serverless-to-handle-huge-queries.html","title":"Use AWS Aurora Serverless to handle huge queries"},{"content":"In Java world, there are different logging frameworks available, e.g: slf4j, log4j, commons logging as well as java standard log, the reason why we have so many log frameworks is because the log module in jdk is not good enough initially. in Python world it's totally different story, most of the time we should use python standard logging module, in grpc-mate, we follow the standard way to do logging as well. log is very important for a running server, developer rely on the log to know what's going on when the server is running, in grpc-mate, I use a yaml file to config the log like below version: 1 formatters: simple: format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s' handlers: console: class: logging.StreamHandler level: DEBUG formatter: simple stream: ext://sys.stdout loggers: __main__: level: DEBUG handlers: [console] propagate: no service.greeter_servicer: level: DEBUG handlers: [console] propagate: no root: level: INFO handlers: [console] propagate: yes we could format the log message format to show datetime followed by logger name , then log leven and the actual log message, we could refer what format available in logrecord-attributes, we could also define the handlers, in this example, I will just out put the log into stdout, it's important we know where the log message comes from, so we could also define differnet loggers and customize the log level for each logger, finally we should define a root logger, if we do not define specific logger, it will reuse the root logger config. after define the log config, we could load the into runtime, I achieve this by putting logic below in the grpc server module, so it's configged before we start grpc server # Create a custom logger with open('logging.yaml', 'r') as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config) now we could define logger for each module, it's recommended to define a logger for each module logger = logging.getLogger(__name__), in grpc-mate I define logger in each servicer greeter_servicer.py product_read_servicer.py product_update_servicer.py when we start the grpc server and try to call the greeter service, the server will output log like below 2019-11-13 19:21:44,320 - __main__ - DEBUG - grpc server started at port 8080 2019-11-13 19:21:49,430 - service.greeter_servicer - DEBUG - get request local ","path":"programing/python/how-to-config-python-log-in-grpc-server.html","title":"How to config python log in Grpc Server"},{"content":"In grpc-mate we have product_read_servicer.py to show how to output large binary stream via data chunk in grpc. protobuf rpc DownloadProductImage(DownloadProductImageRequest) returns(stream DataChunk){ } message DownloadProductImageRequest { int64 product_id = 1; } message DataChunk { bytes data = 1; } Server Side def DownloadProductImage(self, request, context): chunk_size = 1024 image_path = Path(__file__).resolve().parent.parent.joinpath('images/python-grpc.png') with image_path.open('rb') as f: while True: chunk = f.read(chunk_size) if not chunk: break yield DataChunk(data=chunk) it's recommended to use pathlib over os.path as it's follow the OO design, see details from this article thanks to python's simple design, it's much simpler than the Java Version to stream binary files as stream, we just need to open the binary file in binary mode and read the data chunk by chunk, then just yield it, the grpc framework will take care all the other stuff Client Side def test_DownloadProductImage(grpc_stub): faker = Faker() target_image_file = faker.file_name(category=None, extension='png') data_chunks = grpc_stub.DownloadProductImage(DownloadProductImageRequest(product_id=1)) with open(target_image_file, 'wb') as f: for chunk in data_chunks: f.write(chunk.data) original_image_file = Path(__file__).resolve().parent.parent.parent.joinpath('images/python-grpc.png') assert filecmp.cmp(original_image_file, target_image_file) os.remove(target_image_file) first we create a fake image file path to save download image by python faker then call the grpc service, it will return an iterator next we could open the fake image file in write mode and iterator over the data chunk to save the image filee after the fake image file is saved locally , we could use filecmp module to make sure the downloaded file is the same as original image file finally we could delete the temp fake image file as a clean up. ","path":"programing/python/how-to-output-large-binary-stream-via-data-chunk-in-grpc.html","title":"How to output large binary stream via data chunk in Grpc"},{"content":"In grpc-mate we have product_read_servicer.py to show how to do server stream and bi-directional streaming in grpc. protobuf service ProductReadService { //download product by category //used to demo server side stream rpc DownloadProducts (DownloadProductsRequest) returns (stream Product) { } //search product and return all matched products //used to demo simple grpc call rpc SearchProducts (SearchProductsRequest) returns (SearchProductsResponse) { } //calcualte each proeuct sore based on simple rule //used to demo bi directional stream rpc CalculateProductScore (stream Product) returns (stream CalculateProductScoreResponse) { } rpc DownloadProductImage(DownloadProductImageRequest) returns(stream DataChunk){ } } Server Stream def DownloadProducts(self, request, context): with session_scope() as session: result = session.query(DBProduct) \\ .filter(DBProduct.category == request.category) \\ .all() for product in result: yield db_product_to_protobuf_product(product) in order to return a stream of data, the there is no return statement in the server side, it use yield instead, this will make the call to DownloadProducts to return a python generator, grpc framework will take care of call next element of the generator as a stream, it will also handle the end of stream automatially to call the server stream, we could call the grpc method normally, but it will return an iterator, so that in clint side, we could iterator over the server stream def test_DownloadProducts_exist(grpc_stub): faker = Faker() category = faker.name() # save to db with session_scope() as session: for idx in range(5): product = DBProduct(product_name=f'{faker.name()}_{idx}', product_price=Decimal(faker.random_int() / 100), product_status=InStock, category=category) session.add(product) result = grpc_stub.DownloadProducts(DownloadProductsRequest(category=category)) # assert we have 5 items assert len(list(result)) == 5 bi-directional Stream def CalculateProductScore(self, request_iterator, context): for product in request_iterator: yield CalculateProductScoreResponse(product=product, score=int(product.product_price * 2)) bi-directional stream is a combination of client stream and server stream, it will accept a stream of input and out put a stream of data as output, thanks to Python's generator design, this is super simple in python, we could iterator the input stream and just yield result out wen it's ready, the grpc framework will take care of the rest to call the bi-directional service, we could also make a generator function to generate the input paramter and iterate the result from the output parameter, look the example below the call of function product_generator will result in a python iterator,this itertor could be used as parameter of CalculateProductScore, the result of CalculateProductScore is also an iterator, we could iterate the result as well. def product_generator(): for i in range(0, 5): yield Product(product_id=i, product_name=f'product_name_{i}', product_price=i, product_status=InStock,category='category') def test_CalculateProductScore(grpc_stub): product_iterator = product_generator() result = grpc_stub.CalculateProductScore(product_iterator) all_result = list(result) assert len(all_result) == 5 for response in all_result: assert int(response.product.product_price * 2) == response.score ","path":"programing/python/how-to-do-grpc-server-stream.html","title":"How to do Grpc Server Stream"},{"content":"Python Decorator and Java Annotation have the same syntax, when I see Python Decorator the first time, I assume it's behave the same as Java Annotation, but indeed this is wrong Python Decorator is just syntactic sugar for passing a function to another function and replacing the first function with the result, Java Annotation is just a meta data store, it could store additional information about the java artifacts(like class, method), we could have logic based on the java annotations Pytnon Decorator functions are first class object in python language, so we could pass funciton as object as parameter, return function as result, this is the foundation of python decorator. so the code like below def py_decorator(func): def wrapper(): print(&quot;before the function is called.&quot;) func() print(after the function is called.&quot;) return wrapper def greet(): print(&quot;hello!&quot;) greet = py_decorator(greet) is the same as @py_decorator def greet(): print(&quot;hello!&quot;) it's just an syntactic sugar, there are more useful tips about python decorators in real python Java Annotation Java Annotation is just a way to add comments to the java class, or method or attribute, it could avaialbe in java compile time or jvm runtime, to declare a java annotation, we could do like below import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.FIELD) public @interface ReadOnly { } this annotation will define an annotation named ReadOnly, this annotation could be applied on java attribute, the annotation will be avaialble in jvm runtime. after we define the annotation, we could apply this in java code like below Class DBFactory { @ReadOnly private DBProvider dbProvider; } this is like add a comment on attribute dbProvider in jvm that this field is readonly nothing else, if we want any behaviour about this annotation, we have to add logic at runtime to control the behavior , thsi is the foundation of Google Guice, Google Guice heavily rely on java annotation to config the java runtime behavoiur and do dependency injection for us.","path":"programing/python/python-decorator-vs-java-annotation.html","title":"Python Decorator vs Java Annotation"},{"content":"In grpc-mate we have product_update_servicer.py to show how to do client stream upload in grpc. protobuf service ProductUpdateService { //upload product into elastic search , make it so that we could search on it //used to demo client side stream rpc UploadProduct (stream Product) returns (UploadProductResponse) { } } message UploadProductResponse { enum ResultStatus { SUCCESS = 0; FAILED = 1; } ResultStatus result_status = 1; } server side class ProductUpdateServiceServicer(grpc_mate.product_search_engine_pb2_grpc.ProductUpdateServiceServicer): def UploadProduct(self, request_iterator, context): with session_scope() as session: for product in request_iterator: db_product = DBProduct() for k in product.DESCRIPTOR.fields_by_name: setattr(db_product, k, getattr(product, k)) db_product.product_id = None session.add(db_product) return UploadProductResponse(result_status=UploadProductResponse.SUCCESS) from the server side, we will get a request iterator to iterate all the uploaded object, in this case it's a stream of Product messages we could re-use the session_scope to access session object without worrying about session management. as we defined DBProdcut have exactly the same attributes as Product message, we could use code below to copy the attributes between the objects, this is super useful when you have lots of fields in the message db_product = DBProduct() for k in product.DESCRIPTOR.fields_by_name: setattr(db_product, k, getattr(product, k)) client side I will use unit test way to show how to make a stream call in the client, I've already discussed how to use pytest-grpc to test the grpc servicer in this article, I will use the same approach in test_product_update_servicer.py, but there are something different from previous article -- we also use sqlalchemy to do data persist, so we need a way to init the DB and destroy the DB after usage, thanks to pytest's fixture, we could easily achieve this by code below @pytest.fixture(autouse=True, scope='function') def create_schema(): if engine.url.__str__() == 'sqlite:///:memory:': Base.metadata.create_all(engine) yield None Base.metadata.drop_all(engine) create_schema fixture will be exucted before any test method and yield nothing, after the test method complete, it will drop all the tables, so any test method in the module will have a clean db state to use. to call the grpc method, we simply use a list to simulate the iterator then pass to the client call, due to python's iterator design, apart from list, we could write a custom iteerator or generator to have a limitless client stream pass to the grpc server products = [ Product(product_name='product_name_1', product_price=1.0, product_status=InStock, category='category_1'), Product(product_name='product_name_2', product_price=2.0, product_status=InStock, category='category_2')] grpc_stub.UploadProduct(iter(products)) ","path":"programing/python/how-to-do-grpc-client-stream-upload.html","title":"How to do Grpc Client Stream upload"},{"content":"In grpc-mate's protobuf definition, we defined some enum types, when we compile them into python code, it may confuse people on how to use it , especially for the enum defined inside protobuf message standalone enum types enum ProductStatus { InStock = 0; OutStock = 1; } when compile such enum into python code , it will be like below _PRODUCTSTATUS = _descriptor.EnumDescriptor( name='ProductStatus', full_name='ProductStatus', filename=None, file=DESCRIPTOR, values=[ _descriptor.EnumValueDescriptor( name='InStock', index=0, number=0, serialized_options=None, type=None), _descriptor.EnumValueDescriptor( name='OutStock', index=1, number=1, serialized_options=None, type=None), ], containing_type=None, serialized_options=None, serialized_start=196, serialized_end=238, ) _sym_db.RegisterEnumDescriptor(_PRODUCTSTATUS) ProductStatus = enum_type_wrapper.EnumTypeWrapper(_PRODUCTSTATUS) InStock = 0 OutStock = 1 the enum type will be just a number but with it's Own name, we could import the enum like from grpc_mate.product_common_pb2 import InStock nested enum type inside protobuf message message UploadProductResponse { enum ResultStatus { SUCCESS = 0; FAILED = 1; } ResultStatus result_status = 1; } it will be compiled as python code like below _UPLOADPRODUCTRESPONSE_RESULTSTATUS = _descriptor.EnumDescriptor( name='ResultStatus', full_name='UploadProductResponse.ResultStatus', filename=None, file=DESCRIPTOR, values=[ _descriptor.EnumValueDescriptor( name='SUCCESS', index=0, number=0, serialized_options=None, type=None), _descriptor.EnumValueDescriptor( name='FAILED', index=1, number=1, serialized_options=None, type=None), ], containing_type=None, serialized_options=None, serialized_start=188, serialized_end=227, ) _sym_db.RegisterEnumDescriptor(_UPLOADPRODUCTRESPONSE_RESULTSTATUS) lots of people will get confused on how to use ResultStatus enum inside UploadProductResponse, due to python's launage features, we could access such enum as UploadProductResponse.SUCCESS or UploadProductResponse.FAILED. as I said in last section, the enum in python is just a number, we could also access the number like UploadProductResponse.ResultStatus.Value('SUCCESS') -&gt; 0, or access it's name via UploadProductResponse.ResultStatus.Name(0) -&gt; 'SUCCESS'. all the methods are defined in enum_type_wrapper.py.","path":"programing/python/understand-protobuf-enum-in-python.html","title":"Understand Protobuf enum in python"},{"content":"I've spent some time to integrate sqlalchemy into grpc-mate this weekend, I want to use this article to write about my feelings about this tool. Since I come from java world, I may compare it with JPA in java, there are some other ORM libs in python, e.g: Django's ORM,peewee, but sqlalchemy is the most popular one DB Engine and Session sqlalchemy comes with core layer and ORM layer, most of the time for new project ORM layer is recommeded. Engine object in sqlalchemy is in charge of db connection pool management and dialect selection, in java world it's similar to java datasource , e.g: we could use HikariCP to manage database connection pool. to create an Engine, we need at least a db url in format dialect[+driver]://user:password@host/dbname[?key=value..], it also have a rich set of params to contorl the DB engine behavior . it's also recommeded to have only one engine object per db, so in grpc-mate, I create the engine object in the __init__.py for data_store module db_url = os.getenv('db_url', 'sqlite:///:memory:') engine = create_engine(db_url, echo=True) we also get the db_url from system env, if it's not set it will use sqlite in memory db for unit test The Session class is simlar to EntitManagerFactory in JPA, it's in charge of create a session object which act as db connection proxy to communicate with database, in python we could have different kind of Session class, sqlalchemy offer a helper class sessionmaker to create customized Session Class for the end user, we could use code Session = sessionmaker(bind=engine) to create the Session class. Manage Session object lifecycle sqlalchemy has a full document on how to manage session, below is point I got from the long document The Session is very much intended to be used in a non-concurrent fashion, which usually means in only one thread at a time. One Session could have more than one transactions, but it's recommeded for each transaction to be short lived Session object must be closed after usage by default, autoflush is enabled , but autocommit is not enabled for most of the use case in grpc-mate, I would like to have a contextmanager to help me to manage session life cycle for me. @contextmanager def session_scope(): &quot;&quot;&quot;Provide a transactional scope around a series of operations.&quot;&quot;&quot; session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() with the session_scope method, I could use with key word in python to obtain a session object in the safe way, like below to persist a new project object into database faker = Faker() product = DBProduct(product_name=faker.name(), product_price=Decimal(faker.random_int() / 100), product_status=InStock, category=faker.name()) with session_scope() as session: session.add(product) DB Mapping in recenty sqlalchemy version, it provide declarative way to delcar the python DB mapping, the mapping is simlar to JPA's Entity class, but it's kind of simple than JPA Entity, first of all, we need a base class, which could be created by code Base = declarative_base(), then we could use Base as a base class to map a python class to DataBase table., the Column, String are all artifact imported from sqlalchemy from sqlalchemy import Column, SMALLINT, Integer, String, DECIMAL from sqlalchemy.ext.declarative import declarative_base Base = declarative_base() class DBProduct(Base): __tablename__ = 'products' product_id = Column(Integer, primary_key=True) product_name = Column(String(200)) product_price = Column(DECIMAL(10, 2)) product_status = Column(SMALLINT) category = Column(String(50)) to create the db for unit test, we could have pytest fixture to create a new table before each test is running and we also have a safe guard to make sure this only execute in unit test for sqlite db the yield keyword will make sure the init_db function pause on yield and let the unit test run, after unit test complete, it will drop all the database, so that we have a clean state @pytest.fixture(autouse=True, scope='function') def init_db(): if engine.url.__str__() == 'sqlite:///:memory:': Base.metadata.create_all(engine) yield Base.metadata.drop_all(engine_pair) Queries compare with JPA's JPQL, sqlalchemy's query ability is a little limit, Query is the main entrance, we could create Query object by session.query(Entities), then it has a chain style of call stack to filter more field, when we need to do complex queries, the sqlalchemy query will be very complex and not as clean as JPQL, I think I will switch to use sqlalchemy core to use raw sql if I need complex sql query, I may change my mind when I discover more.","path":"programing/python/sqlalchemy-makes-python-orm-easy.html","title":"SQLAlchemy makes Python ORM easy"},{"content":"I've described how to bootstrap a simple grpc server in python,now it's time to test the grpc service. I'm using pytest-grpc to write unit test for grpc service. pytest-grpc is a pytest plugin, it's leverage the power of pytest fixtures to make unit test for grpc service very easy. to write unit test for grpc, there are 3 specific pytest fixtures to implement, it's recommended to import artifacts within the fixture method to avoid package confilict grpc_add_to_server the test need to know how to add grpc servicer into grpc server, this fixutre will return the generated funciton to add grpc servicer to grpc server @pytest.fixture(scope='module') def grpc_add_to_server(): from grpc_mate.helloworld_pb2_grpc import add_GreeterServicer_to_server return add_GreeterServicer_to_server grpc_servicer the test need to know what servicer instance need to add to grpc server, this fixture methjod should return a servicer instnace @pytest.fixture(scope='module') def grpc_servicer(): from service.greeter_servicer import GreeterServicer return GreeterServicer() grpc_stub_cls the test need to know which client stub class to use to init the grpc stub instance @pytest.fixture(scope='module') def grpc_stub_cls(grpc_channel): from grpc_mate.helloworld_pb2_grpc import GreeterStub return GreeterStub after we provide these information, pytest-grpc will do the rest for us, it will take care of the grpc server/ channel's creation and termination, it will help us to construct the client stub, what we need to do is just focus on write meaningful unit test to test out grpc logic, like below def test_SayHello(grpc_stub): hello_request = HelloRequest(name='ivan') response = grpc_stub.SayHello(hello_request) assert response.message == f'hello {hello_request.name}' see the full example from test_greeter_servicer.py grpc-python do have it's own module to test grpc server, but I find the api is very confusing and the test code is not easy to maintain, so it's not recommended to use grpc-python's offical test module.","path":"programing/python/write-unit-test-for-grpc-with-pytest-and-pytest-grpc.html","title":"Write unit test for grpc with pytest and pytest-grpc"},{"content":"I finally got time to put my focus on my most popular open source project grpc-mate, an enterprise ready micro service project base on gRPC. the java based project was completed about 1.5 years ago, now it's time to develop a python based project to demostrate the best practice in python. I find it's a good way for me to learn a new things. before I could write it out, I will have to understand it very well. dependency setup to setup a virtual env for python development, please checkout this article. grpc depends on protobuffer to serialize and deserialize message,so we need both grpc and protobuf as dependencies in the project pipenv install grpcio protobuf, we also need grpcio-tools to generate grpc python stubs, it will generate both client and server stubs in python, so we will need to issue command pipenv install grpcio-tools generate grpc python stubs we could re-use the protobuffer definitions in grpc-mate project, to use it in grpc-mate-python project we could create a folder under grpc-mate named protobuffers and create symlink of grpc_mate and google into this folder so that we will have only one place to manage all the grpc api definition. we could define package structure of protobuf for different language. like example below, we could define the java package and go_package as different package name in it's own language, syntax = &quot;proto3&quot;; option java_package = &quot;io.datanerd.generated.es&quot;; option java_multiple_files = true; import &quot;grpc_mate/product_common.proto&quot;; import &quot;google/api/annotations.proto&quot;; option go_package = &quot;datanerd&quot;; but python is doing other way, it's respecting the protobuf package structure, the python package will be the same as the protobuf package structure, in our case our user defined protobuf will generate python package in package grpc_mate and the google protobuf will generate python files in package google, to generate the python stubs, we could issue command like below rm -fR grpc_mate/* &amp;&amp; \\ rm -fR google/* &amp;&amp; \\ python -m grpc_tools.protoc -Iprotobuffers --python_out=. --grpc_python_out=. protobuffers/grpc_mate/*.proto protobuffers/google/api/*.proto &amp;&amp; \\ touch grpc_mate/__init__.py touch google/__init__.py touch google/api/__init__.py we are touching __init__.py for each package to make it backwards compatible, in python 3.6 this is optional. it is also a good idea to place the auto generated python code in it's own package so that we are confident to re-generate them. implement the grpc service grpcio-tools already generate the server stubs in xxxx_pb2_grpc to implement GreeeterSerevice, we could create a class which extends from grpc_mate.helloworld_pb2_grpc.GreeterServicer and override SayHello function like below, isn't it easy? we could access passed in parameter from request import logging import grpc_mate.helloworld_pb2 import grpc_mate.helloworld_pb2_grpc logger = logging.getLogger(__name__) class GreeterServicer(grpc_mate.helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): logger.debug(f&quot;get request {request.name}&quot;) return grpc_mate.helloworld_pb2.HelloReply(message=f&quot;hello {request.name}&quot;) add grpc service and start grpc server to create a grpc server we will need to pass grpc server a thread pool to use for rpc handlers server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) add the service to the grpc server grpc_mate.helloworld_pb2_grpc.add_GreeterServicer_to_server(GreeterServicer(), server) tell grpc server which port it could listen to, if port not specified grpc will choosee a random port and return it server.add_insecure_port('[::]:8080') start the grpc server server.start() let server to wait there to accpet incomming call server.wait_for_termination() to put everything together,the code is like below def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) grpc_mate.helloworld_pb2_grpc.add_GreeterServicer_to_server(GreeterServicer(), server) server.add_insecure_port('[::]:8080') server.start() logger.debug('grpc server started at port 8080') server.wait_for_termination() if __name__ == '__main__': serve() test call to grpc server I will write another article to describe how to do grpc's unit test, now I just use BloomRPC to the the call manually load the protobuf into BloomRPC open Greeter Service click SayHello node in the left panel input server address as localhost:8080 in the left panel, input parameter as &quot;name&quot;:&quot;Ivan&quot; click &quot;go&quot; button in middle of the panels get the message as &quot;message&quot;: &quot;hello Ivan&quot; write grpc client to call the server def integration_test_SayHello(): from grpc_mate.helloworld_pb2_grpc import GreeterStub channel = grpc.insecure_channel('localhost:8080') stub = GreeterStub(channel) hello_request = HelloRequest(name='local') response = stub.SayHello(hello_request) assert response.message == f'hello {hello_request.name}' ","path":"programing/python/setup-grpc-server-project-in-python.html","title":"Setup Grpc Server project in Python"},{"content":"I'm working on an API update today, I will need to add more fields in both the request parameter and api response. thanks to GRPC's elegant design , I could update the API fields and apply to production without worrying about any backward compatibility. GRPC's message is based on protocol-buffers, a language-neutral, platform-neutral, extensible mechanism for serializing structured data. all the GRPC APIs are designed in protocol-buffers, a hello world type of grpc api definition could be found in one of my project grpc-mate's protobuffer package helloworld.proto syntax = &quot;proto3&quot;; option java_package = &quot;io.datanerd.generated.helloworld&quot;; option java_multiple_files = true; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user's name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } the request and response are just one protobuf message. with this design, I could add more fields in the request and response, but the API still remains the same. for example, I want to add a new field in HelloReply to indicate response date, I could do the following // The response message containing the greetings message HelloReply { string message = 1; string today = 2; } the GRPC client and the server normally does not deploy together, we could update server first to include today field, when client have not update the protobuf file, even server response HelloReply contains today field, due to client has not updated, it still work, but just ignroe today field in the HelloReply message. this feature is super useful for the application which is already in prodcution mode, we could update server first anad the server clould support client call no mater it update the protobuf or not. there are some other benefits we could get from GRPC, I will discuss them in other articles","path":"programing/java/grpc-makes-api-update-easier.html","title":"Grpc makes api update easier"},{"content":"When I develop kubernetes-service-dns-exporter, I'm using boto3 to communicate with AWS for route53 operation. I find out Moto when trying to research on how to mock aws services. Moto is a fantastic library to help me to mock aws servcie API and test my code logic without cost and it's very easy to use, it has implement most of the common AWS api endpoint. moto could be simply installed by pip install moto. mock aws client use python decorator @mock_xxx , xxx is the service name you want to mock, e.g @mock_s3 will mock all s3 related operation in boto to use moto, see below example from moto @mock_s3 def test_my_model_save(): # Create Bucket so that test can run conn = boto3.resource('s3', region_name='us-east-1') conn.create_bucket(Bucket='mybucket') model_instance = MyModel('steve', 'is awesome') model_instance.save() body = conn.Object('mybucket', 'steve').get()['Body'].read().decode() assert body == 'is awesome' work with pytest we could leverage pytest's fixture feature to make sure the boto3 client does not connecting to real aws service by @pytest.fixture(scope='function') def aws_credentials(): &quot;&quot;&quot;Mocked AWS Credentials for moto.&quot;&quot;&quot; os.environ['domain_name'] = 'test.com' os.environ['aws_access_key_id'] = 'testing' os.environ['aws_secret_access_key'] = 'testing' we could also use pytest's fixture feature to generate the route53 client for the test method @pytest.fixture(scope='function') def route53(aws_credentials): with mock_route53(): yield boto3.client('route53') so in the test method, I do not need to use the mock_route53 decorator, I could write the test case like below def test_route53(route53): assert os.getenv('domain_name') == 'test.com' resp = route53.create_hosted_zone(Name=os.getenv('domain_name'), CallerReference=str(hash('xxx'))) zone_id = resp['HostedZone']['Id'] os.environ['hosted_zone_id'] = zone_id dns_name, record = route53_dns('CREATE', 'test_service', '127.0.0.2') assert dns_name == 'kube-test_service.test.com' assert record['ResponseMetadata']['HTTPStatusCode'] == 200 ","path":"programing/python/use-moto-to-test-aws-service-in-python.html","title":"Use moto to test AWS service in Python"},{"content":"I've described on how to develop Kubernetes Operator in Python in last article, now it's time to deploy it into Kubernetes Cluster, I'm using AWS EKS, but the deployment method is standard kubernetes way so it could be deployed to GKE or other Kubernetes cluster as well. step 1 expose python dependencies we could achieve this by command pipenv lock -r &gt; requirements.txt step 2 build docker image the content of the image is as simple as below FROM python:3.7 LABEL maintainer=&quot;email2liyang@gmail.com&quot; COPY kube_service_dns_exporter.py /kube_service_dns_exporter.py COPY requirements.txt /tmp # install extra dependencies specified by developers RUN pip install -r /tmp/requirements.txt CMD kopf run /kube_service_dns_exporter.py --verbose then we could build the docker image and push it to docker hub docker build -t email2liyang/kube_service_dns_exporter:latest . docker push email2liyang/kube_service_dns_exporter:latest the docker image is ready to use in docker hub https://cloud.docker.com/u/email2liyang/repository/docker/email2liyang/kube_service_dns_exporter step 3 define rbac for the operator this is important step , because the operator need some specific permission to communicate with kubernetes api, Kopf is using pykube-ng to communicate with kubernetes api. you could checkout my rbac details from rbac.yml. in kubernetes-service-dns-exporter, I'm just interested in kubernetes service's creation and deletion event and I've defined a service account named kopf to do that step 4 define kubernetes secret it's a good practice to place sensitive data into kubernetes secret and just refrence it in deployment, in my case I'm putting my route53 zone id and aws access key and secret key into kubernetes secret, this is just an example of how the kubernetes secret is organised step 5 define kubernetes deployment the full deployment yaml file could be found in github. in the deployment yaml file, I'm using the service account kopf to make sure pod has enough permission to communicate with kubernetes api. it's also recommended to set replicas value as 1 because if more than 1 operator for the samee event exist in the cluster, it may result in some un-expected behaviour. step 6 deploy kubernetes artifact into the cluster just issue command kubectl apply -f yaml/, we are done, we could check the pod are running well kubectl get po -n tool NAME READY STATUS RESTARTS AGE kube-service-dns-exporter-dd8bc57cb-6gf52 1/1 Running 0 1h and new event are attached to exist service kubectl describe svc redis -n tool Name: redis Namespace: tool Labels: app=redis role=master tier=backend Annotations: kopf.zalando.org/last-handled-configuration={&quot;spec&quot;: {&quot;ports&quot;: [{&quot;protocol&quot;: &quot;TCP&quot;, &quot;port&quot;: 6379, &quot;targetPort&quot;: 6379, &quot;nodePort&quot;: 30702}], &quot;selector&quot;: {&quot;app&quot;: &quot;redis&quot;, &quot;role&quot;: &quot;master&quot;, &quot;tier&quot;: &quot;backe... kubectl.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;app&quot;:&quot;redis&quot;,&quot;role&quot;:&quot;master&quot;,&quot;tier&quot;:&quot;backend&quot;},&quot;name&quot;:&quot;redis&quot;,&quot;namespace&quot;:&quot;... Selector: app=redis,role=master,tier=backend Type: NodePort IP: 172.20.53.35 Port: &lt;unset&gt; 6379/TCP TargetPort: 6379/TCP NodePort: &lt;unset&gt; 30702/TCP Endpoints: 10.31.4.218:6379 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Logging 5m kopf creating dns for service redis which point to ip 172.20.53.35 Normal Logging 4m kopf All handlers succeeded for creation. Normal Logging 4m kopf created dns kube-redis.vipmind.me point to 172.20.53.35 Normal Logging 4m kopf Handler 'create_fn' succeeded. we could also observe that the dns record is created in route53 Enjoy!","path":"infra/kube/deploy-kubernetes-operator-into-aws-eks-cluster.html","title":"Deploy Kubernetes Operator into AWS EKS Cluster"},{"content":"Kubernetes has an operator pattern, the operator is aimed to be developed by domain expert to leverage the kubernetes automation and domain skills to resolve speicific problems. CoreOS also developed a operator framework to help to develop kubernetes operator. but unfortunately it's written in go language and I don't know go language. luckly I meet Kopf when reading some technical articles, it's an kubernetes opeerator framework written in python and super easy to use. I'm in charge of a complex GRPC microservice based data processing system deployed to AWS EKS, the Kubernetes cluster is doing a fantanstic job on the service orchestration, however one challenge I'm facing is the service discovery. all the GRPC are within the same EKS cluster and I rely on kubernetes service as a static endpoint to act as grpc endpoint, this works very well , everytime I redeploy any microservice, I don't need to care about what's the pod's ip, the service will balance the request to the live pods out of box, but the service is just an random ip within kubernetes cluster, it's not easy to to understand which ip map to which service, so we assign each service ip to a meaning full dns name and use the dns name in the code as the grpc service endpoint. for example, I have a service hello-world like below NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-world NodePort 10.108.222.86 &lt;none&gt; 8000:31022/TCP 15s every time I deploy a service into kubernetes cluster, I will have to add a dns record into route53 kube-hello-world.vipmind.me -&gt; 10.108.222.86 before I write the kubernetes operator, I have to do it manually or write a terraform script to apply it. I always wondering if there is any way to do it automatically. when I meet Kopf I think it's time to open the door.my requirement is simple when a new service is deployed to my eks cluster, I want to create a new dns entry kube-${service-name}.vipmind.me point to the service ip when a service is deleted from my eks cluster, I want to delete the dns entry kube-${service-name}.vipmind.me point to the service ip I could achieve this by onle 87 lines of python code in kube_service_dns_exporter.py , let's break them down to explain the details use decorator @kopf.on.create('', 'v1', 'services') to tell kopf that I want to monitor api group '' or 'core' , version 'v1', kind 'services', when the any service is created the function create_fn will be called. the parameters in the function are all keyword based parameter, and I could all the service metadata, spec etc from the parameter. inside function create_fn I could extract the servcie name from meta['name'] and service ip from spec['clusterIP'] I could pass service_name and service_ip field into function route53_dns to create the dns record in route53 in function route53_dns, I'm using boto3's route53 client to create the dns record , boto3 is stable now and it's recommended to use boto3 over boto if we are developing new project after create the dns record I could use get_change funciton to monitor the the process. we could also use logger parameter from create_fn to attach event into kubernetes service event list logger.info(f&quot;deleted dns {result[0]} point to {spec['clusterIP']}&quot;) will attach an event like below kubectl describe svc hello-world -n test Name: hello-world Namespace: test Labels: &lt;none&gt; Annotations: kopf.zalando.org/last-handled-configuration={&quot;spec&quot;: {&quot;ports&quot;: [{&quot;name&quot;: &quot;web&quot;, &quot;protocol&quot;: &quot;TCP&quot;, &quot;port&quot;: 8000, &quot;targetPort&quot;: 8000, &quot;nodePort&quot;: 31022}], &quot;selector&quot;: {&quot;app&quot;: &quot;hello-world&quot;}, &quot;clusterIP... kubectl.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;hello-world&quot;,&quot;namespace&quot;:&quot;test&quot;},&quot;spec&quot;:{&quot;ports&quot;:[{&quot;name&quot;:&quot;web&quot;,&quot;port&quot;:8000,&quot;p... Selector: app=hello-world Type: NodePort IP: 10.108.222.86 Port: web 8000/TCP TargetPort: 8000/TCP NodePort: web 31022/TCP Endpoints: 172.17.0.4:8000 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Logging 41s kopf creating dns for service hello-world which point to ip 10.108.222.86 Normal Logging 4s kopf route53 record {'ResponseMetadata': {'RequestId': '4585fae2-2b01-4d5e-b77b-72e628ed1860', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '4585fae2-2b01-4d5e-b77b-72e628ed1860', 'content-type': 'text/xml', 'content-length': '336', 'date': 'Mon, 07 Oct 2019 09:34:11 GMT'}, 'RetryAttempts': 0}, 'ChangeInfo': {'Id': '/change/C2QQJVYW684DNC', 'Status': 'INSYNC', 'SubmittedAt': datetime.datetime(2019, 10, 7, 9, 33, 36, 564000, tzinfo=tzutc()), 'Comment': 'CREATE dns record for kube-hello-world.vipmind.me point to 10.108.222.86'}} Normal Logging 4s kopf created dns kube-hello-world.vipmind.me point to 10.108.222.86 Normal Logging 4s kopf All handlers succeeded for creation. Normal Logging 4s kopf Handler 'create_fn' succeeded. use decorator @kopf.on.delete('', 'v1', 'services') to tell kopf that I want to monitor api group '' or 'core' , version 'v1', kind 'services', when the any service is deleted the function delete_fn will be called. to test the operator locally we could run command below to start the operator locally and use minikube to do the integration test export hosted_zone_id=xxxxxxxxxxxx &amp;&amp; \\ export domain_name=vipmind.me &amp;&amp; \\ export domain_prefix=kube &amp;&amp; \\ export aws_access_key_id=xxxxxxxxxxxxxxx &amp;&amp; \\ export aws_secret_access_key=xxxxxxxxxxxxxxxxxxxx &amp;&amp; \\ kopf run kube_service_dns_exporter.py --verbose Check out this article to see how to deploy it into kubernetes cluster","path":"infra/kube/writing-kubernetes-operator-in-python-with-kopf.html","title":"Writing Kubernetes Operator in Python with Kopf"},{"content":"Today I got a bunch of messages from the other end of the world , my co-workers are all located in the other end of the world, I work when they sleep, they work when I sleep, we only have a little time cross to discuss issues. today's issue is interesting, we are using the approach I described in Managing EKS Worker nodes dynamically by AWS Auto Scaling Groups to manage the EKS worker nodes but with more fine control, we want to terminate worker nodes in ASG by selecting label, there is no issue on the logic, but there is a strange behaviour when trying to terminate some node, when we terminated some nodes with selected label, it terminate that node, but it also terminate some other EKS worker nodes unexpectedly. When I was called to check this issue I suddenly remember that ASG has a feature named AZRebalance in the Scaling Process, words in short is that if the ASG is cross different AZ(Availability Zone), by default it will try to rebalance the ec2 instnace in the ASG to make sure the ec2 instances are balanced across AZ, below is the official document from AWS This is exactly what happend for us, when we terminate any ec2 worker nodes in ASG and ASG detect it's not AZBalaneced, so ASG will try to promote more instnace and terminate other instnaces to make the ASG AZBalanced. to fix this issue is easy, we could suspended the AZRebalance process for selected ASG ","path":"infra/aws/asg-azrebalance-will-impact-eks-node.html","title":"ASG AZRebalance will impact EKS node"},{"content":"Recently I spend some time on pySpark to see if it could help our team to resolve some of the problems we are facing on apache airflow. airflow is a great tool for scheduling, triggering and monitoring etl jobs, but itself is not a data processing tool, we are trying to use apache airflow to act as both scheudling tool and data processing tool, here comes a problem, we have only one airflow cluster anytime airflow get redeployed, some running jobs got a chance to get interrupted. there are some ways to overcome this issue, but still running differnt jobs on the same cluster is not a good idea. it's ideal to have each data processing job running on it's dedicated cluster and isolated from each other. Spark support to deploy to kubernetes out of box from v2.3, and we could create dedicated spark cluster for any single spark job. as our team prefer python as the first development language , so pySpark looks like a perfect tool to look into. pySpark image spark team does not provide pre-build docker image for spark, but it provide a docker-image-tool.sh to build an push spark images to docker registory, I've built an image email2liyang/spark-py and publish it to github for public use, we could also use this image as a base image and custome spark-py image to meet team's own need. for example, install additional software packages or install some python dependencies, I will take about this later. pySpark project setup I use pipenv to manage my python project dependencies,you could check this article for how to use pipenv for details. there could be 2 main folders in the project vip_udfs: hold the business logic for all the jobs, different logic could be separated in different python package, it mostly expose as user defined function(udf), the udf could be tested as standard pthon function as well vip_jobs: hold the main pySpark jobs , it in charge of create the spark job and set the dependency path etc. it will call udf defined in package vip_plugins python dependency management the python dependendcies is the python code which is expected to be in each worker's runtime python path, so that the pySpark job could invoke the funcitons provided from the dependencies. there are 3 kinds of dependencies the 3rd party python dependencies we could use pipenv to install the 3rd party python dependencies into local env and expose them via pipenv lock -r &gt; requirements.txt, when we build custom docker image based on spark-py, we could install these dependencies. the UDF defined in folder vip_udfs these are the user defined functions which support the main pySpark jobs,these python module could be packaged as a zip file and send to a shared volume in the spark cluster, in my case I've provision an AWS EFS and mout it as shared volume across all the pySpark worker pods, let's assume vip_udfs is packaged as vip_udfs.zip and copied to path /var/efs/pyspark/job-station/vip_job1/vip_udfs.zip to make the spark worker could access this type of python dependencies, in our pySpark job, we need to write like below spark_session = SparkSession \\ .builder \\ .appName(&quot;vip_job1&quot;) \\ .getOrCreate() spark_session.sparkContext.addPyFile('/var/efs/pyspark/job-station/vip_job1/vip_udfs.zip') the 3rd party jars spark is written in Scala and running on JVM, so the core spark runtime is running in JVM , we could also utilise some 3rd party java lib to help us on some tasks in the flow. e.g: load data from mysql through mysql jdbc. to do this , we could copy mysql_mysql-connector-java-5.1.44.jar into a shared location, e.g: /var/efs/pyspark/jars/mysql_mysql-connector-java-5.1.44.jar and in spark-submit, we could delcaer the additional jar via --jars options. sumbit pyspark job into kubernetes below is the pySpark architecture on kubernetes, we could use spark-submit utility to submit the pySpark job into kubernetes cluster, it will create a driver pod first, and the driver pod will also communicate with api server to create executor pod as defiend, the driver pod also in charge of all the job pod's life cycle, we only need to manage the driver pod. below is a sample command to submit a pySpark job into kubernetes cluster with explanation on each option spark-submit \\ # the kubernetes api point, we need to config kubectl to be able to access this --master k8s://https://xxxxx.aa.us-east-1.eks.amazonaws.com:443 \\ # declare we use cluster mode --deploy-mode cluster \\ # the 3rd party jar which could be access from all the spark runtime --jars /var/nfs/pyspark/jars/mysql_mysql-connector-java-5.1.44.jar \\ # the udf files which could be access from the driver node --py-files local:///var/efs/pyspark/job-station/vip_job1/vip_udfs.zip \\ # spark could run on under dedicated service account # an rbac config is needed for this service account to access kubernetes api --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sc \\ # the spark could run all the spark job in it's own namespace --conf spark.kubernetes.namespace=spark \\ # when we build custom docker image # it could be stored in a private docker registery e.g:gcr # this secret name is used to pull the image from private docker registry --conf spark.kubernetes.container.image.pullSecrets=vip-mind-docker-registry \\ # the custom pyspark image to pull from docker registry --conf spark.kubernetes.container.image=us.gcr.io/vip-mind/my-pyspark:latest \\ # specify how offten you want to pull the new image --conf spark.kubernetes.container.image.pullPolicy=Always \\ # the job name --name vip_job1 \\ # how many executors you want the driver to create # in this case the driver pod will create 10 executor pods to process the data --conf spark.executor.instances=10 \\ # currently spark support both python 2 and python3 # but spark will drop python 2 in a short time # so it's recommended to use python 3 --conf spark.kubernetes.pyspark.pythonVersion=3 \\ # we could mount additional pv into the driver pod, e.g: the efs # so that driver could access the shared efs volume --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc.mount.path=/var/efs \\ # do we mount the volume as readOnly? --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc.mount.readOnly=false \\ # which pvc we should claim from for the efs --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc.options.claimName=efs \\ # we could mount additional pv into the driver pod, e.g: the efs # so that driver could access the shared efs volume --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc.mount.path=/var/efs \\ # do we mount the volume as readOnly? --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc.mount.readOnly=false \\ # which pvc we should claim from for the efs --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc.options.claimName=efs \\ # we could also assign the spark to run on specific ec2 nodes --conf spark.kubernetes.node.selector.beta.kubernetes.io/instance-type=m5.large \\ # we could also set the how much cpu or memory when launch the executor pod --conf spark.kubernetes.executor.request.cores=300m \\ # the py spark main job file local:///var/efs/pyspark/job-station/vip_job1/main.py ","path":"data/spark/deploy-pyspark-jobs-into-kubernetes-with-python-dependencies.html","title":"Deploy pySpark jobs into kubernetes with python dependencies"},{"content":"EKS could mount EBS as Persistent Volumes in EKS cluster, we could also declear PersistentVolumnClaim to provision PV dynamically , this is great and we could treat EBS as our EKS disk pool to claim almost unlimited disk inside kubernetes pod. EBS also have different volume types, each volume type could support different type of IOPS. some EC2 instance could also have Instance Store Volumes, these volumes are in the same physical server as the ec2 instance, so they usually have better performance then EBS, the draw back of Instance Store Volumes is that when EC2 instance get rebooted or shutdown, ec2 instnace may move to other host machine and the Instance Store Volumes will be lost. if we are ok about this draw back, the Instance Store Volumes could give us best IO and no additional charge of EBS. I have a use case which Instance Store Volumes fits us very well, I will need to launch a search server in a pod, the search server will first download the indexed file into local disk and servie online query, this is very IO sensitive service, since we could place the index file into a safe place, e.g: s3 and download it into local disk, I could use Instance Store Volumes to be mount into my pod to keep the index copy and the searchd could also access Instance Store Volumes to do real time query with the best IO performance. below is the steps how I provision the Instance Store Volumes and use it in my pod. Different EC2 instance will have different Instance Store Volumes, I will use r3.2xlarge as an example Create an ASG Launch Configuration I've already described how to use ASG to manage the worker nodes dynamically , when we create an ASG Launch Configuration , in the userdata section we need some command to config the ec2 nodes sbin/mkfs -t ext4 /dev/xvdb /bin/mkdir /media/ephemeral0 /bin/mount /dev/xvdb /media/ephemeral0 /bin/chown root:root -R /media/ephemeral0 these commands should be placed above /etc/eks/bootstrap.sh Create ASG based on ASG Launch Configuration see this article for details when the new ec2 instance managed by the defined ASG join EKS cluster, we could already have the Instance Store Volumes ready to use lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 20G 0 disk └─xvda1 202:1 0 20G 0 part / xvdb 202:16 0 150G 0 disk /media/ephemeral0 use /media/ephemeral0 in yaml file we could declear hostPath type of volumes in kubernetes like below to mount /media/ephemeral0 in your pod container on /media/disk0 apiVersion: v1 kind: Pod metadata: name: pv-recycler namespace: default spec: restartPolicy: Never volumes: - name: vol hostPath: path: /media/ephemeral0 containers: - name: pv-recycler image: &quot;k8s.gcr.io/busybox&quot; command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo 'hello'&quot;] volumeMounts: - name: vol mountPath: /media/disk0 now in the pod pv-recycler I could write any logic to use disk /media/disk0, it will use Instance Store Volumes under the hood and offer the best IO performance.","path":"infra/aws/how-to-use-ephemeral-disk-from-ec2-instance-in-eks.html","title":"How to use ephemeral disk from EC2 instance in EKS"},{"content":"It's very common for a company to host a private maven reposiroty, so that the development team could share java artifacts, there are also lots of solutions for self hosted maven repository, e.g: Nexus from sonatype.com or JFrog Artifactory, these are cool products, but most of the time we just want a place to store the artifacts and could be downloaded by internal development team. if we could leverage the online storage service to act as our maven repository then we do not need to spend effort to manage the the repository by ourselvs and do not need to worry about the disk issue backup etc. here comes the solution to host maven reposority on AWS S3 setup s3 as maven repository I have a terraform script to do this , the whole script could be found under my github account. create the s3 bucket as my maven repository resource &quot;aws_s3_bucket&quot; &quot;corp_maven_repo&quot; { bucket = &quot;${var.bucket_name}&quot; versioning { enabled = false } lifecycle { prevent_destroy = false } } create 2 users to access the s3 bucket, one user is called ro user which is used by the development team to download the artifact, the other user is called rw user which could be used by CI server to upload artifact to s3 # we need a service account user resource &quot;aws_iam_user&quot; &quot;s3_repo_user_rw&quot; { name = &quot;srv_${var.bucket_name}_rw&quot; } # generate keys for service account user resource &quot;aws_iam_access_key&quot; &quot;s3_repo_user_keys_rw&quot; { user = &quot;${aws_iam_user.s3_repo_user_rw.name}&quot; } # we need a service account user resource &quot;aws_iam_user&quot; &quot;s3_repo_user_ro&quot; { name = &quot;srv_${var.bucket_name}_ro&quot; } # generate keys for service account user resource &quot;aws_iam_access_key&quot; &quot;s3_repo_user_keys_ro&quot; { user = &quot;${aws_iam_user.s3_repo_user_ro.name}&quot; } grant read only and read write permission to the ro and rw user, you could check the detail code from https://github.com/email2liyang/terraform-mate/blob/master/unit2/main.tf#L38-L99 have the terraform to output ro and rw user's access key and secret key //the access key output &quot;iam_access_key_id_rw&quot; { value = &quot;${aws_iam_access_key.s3_repo_user_keys_rw.id}&quot; } //the access key secret output &quot;iam_access_key_secret_rw&quot; { value = &quot;${aws_iam_access_key.s3_repo_user_keys_rw.secret}&quot; } //the access key output &quot;iam_access_key_id_ro&quot; { value = &quot;${aws_iam_access_key.s3_repo_user_keys_ro.id}&quot; } //the access key secret output &quot;iam_access_key_secret_ro&quot; { value = &quot;${aws_iam_access_key.s3_repo_user_keys_ro.secret}&quot; } use s3 as repository in gradle project Gradle support s3 as maven repository out of box, so in build.gradle file, we could simply specify repositories as below repositories { mavenLocal() mavenCentral() jcenter() maven { credentials(AwsCredentials) { accessKey &quot;xxxxx&quot; secretKey &quot;xxxxx&quot; } url 's3://corp_maven_repo/maven/snapshots/' } maven { credentials(AwsCredentials) { accessKey &quot;xxxxx&quot; secretKey &quot;xxxxx&quot; } url 's3://corp_maven_repo/maven/releases/' } } use s3 as repository in maven project maven does not support s3 out of box, but there are plugins to support it, e.g: https://github.com/seahen/maven-s3-wagon. we could config the extention first &lt;build&gt; &lt;extensions&gt; &lt;extension&gt; &lt;groupId&gt;com.github.seahen&lt;/groupId&gt; &lt;artifactId&gt;maven-s3-wagon&lt;/artifactId&gt; &lt;version&gt;[S3 Wagon Version]&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;/build&gt; then add the maven repository in the repositories &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;corp-releases&lt;/id&gt; &lt;url&gt;s3://corp_maven_repo/maven/releases&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;daily&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; to authenticat to s3, we need to a section in ~/.m2/settings.xml &lt;server&gt; &lt;id&gt;corp-releases&lt;/id&gt; &lt;username&gt;xxxx&lt;/username&gt; &lt;password&gt;xxxx&lt;/password&gt; &lt;/server&gt; now we could use s3 as maven repository for both of maven and gradle projects","path":"infra/aws/-how-to-use-s3-as-a-maven-repository.html","title":" How to use S3 as a maven repository "},{"content":"In last article I described how to deploy apache airflow into kubernetes, in this article I'm going to talk about how to organize the airflow project in an efficient way so that it's easy to mantain by both development team and devOps team Python Dependency Management Airflow is written in python if you start a fresh project, I recommend to use python 3, as python 2 will be end of life in 2020. I also recommand to use pipenv to manage python dependencies rather than pip, you could refrence this article to see how to setup python development env with pypenv and pipenv. as this article is written, I'm using python 3.6 and airflow 1.10.2 to build my airflow jobs. to install airflow by pipenv, we need a system env AIRFLOW_GPL_UNIDECODE=yes before run pipenv install apache-airflow, developers will have their own flexibility to install any python dependencies locally and in the end, we could have a command to export all the dependencies into requirements.txt . the command to export python dependencies is AIRFLOW_GPL_UNIDECODE=yes pipenv lock -r &gt; requirements.txt after we have requirements.txt , we could list it as a build artifact and install them all in the customised airflow image Airflow Project Structures Airflow's main concept is called Directed Acyclic Graph (DAG), it's recommended to have a folder named dags to hold all the dags for the airflow project and keep the dag as simple as possible, many small dags are better than a large complex dag to maintain Airflow also have a plugins concept , it's recommended to keep complex logic as a plugin and reuse it in DAG. apart from these 2 folder, airflow-kubernetes offer some super handy location to help developers to resolve some generic problems, I recommend to have a config folder under project root folder in config folder we could have a folder named init, I've implemented a SystemV sytle init script to help developer to init airflow system after airflow webserver started, these script will be run under linux user name airflow, developer could leverage this system to init airflow config e.g: connections. in config folder, we could also have a folder named super_init, the script under super_init could be executed under linux username root, this is super useful in kubernetes env, because the ip of any airflow work will get changed after each deployment and airflow webserver depends on airflow work's hostname to communicate with each work to pull task logs, we could use this system to help airflow webserver to resolve each workre's hostname by having a script like content below under super_init dir kubectl get po -n airflow -o wide \\ | grep airflow-worker \\ | awk '{printf(&quot;%s\\t%s\\n&quot;,$6,$1)}'&gt;&gt; /etc/hosts it's better to have a Makefile in the root of the project folder so that developer could put some useful command in the Makefile and it's also a good way of code as document developer could have their own flexibility to put any other fodlers in the project Docker Image Management The image I provide in airflow-kubernetes is a generic airflow image, it just provide python and airflow runtime on linux box. I recommand to maintain a custome airflow image which extends from docker-airflow and push to it's own private docker registry, e.g: GCR or ECR because developer may want to install custom software in the image developer may want need a way to install 3rd party python dependencies into the image which is specified in requirements.txt developer may want to load some custom config into ariflow config developer may want to put some credentials into the image based on business requirement developer may also want to have a way to version their own airflow deployment rather than using a base image. in a multi workers airflow cluster, it is required that all the python code for airflow are exactly the same across the node, so it's good to build airflow jobs and package all the python code as an artifact , when we build a new image , we could pull the latest artifact and put them into related dirs, then when we deploy the image into kubernetes cluster, we are sure that the python code are exactly the same across different airflow worker nodes to maintain a custom airflow image is easy, just have a Dockerfile start with FROM email2liyang/docker-airflow:1.10.2, then you could start to customise your own image to deploy your own business logic into your airflow cluster.","path":"programing/python/organize-airflow-project-in-an-efficient-way.html","title":"Organize Airflow Project in an Efficient Way"},{"content":"Apache airflow is a powerful tool in data processing world, below is the official airflow introduction Airflow is a platform to programmatically author, schedule and monitor workflows. Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Apache airflow support Celery as its task queue to distribute tasks across different worker nodes. it will be great if we could deploy apache airflow into kubernetes cluster and let kubernetes to manage the celery cluster with simple command kubectl scale --replicas=N airflow-worker, that's why my project airflow-kubernetes was born. I'm re-using an popular airflow docker image, but modify it to support deploy airflow into kubernetes with both LocalExecutor and CeleryExecutor. the LocalExecutor is useful for local development, the CeleryExecutor is useful for production deployment with as many celery works as you want in kubernetes cluster. there are some difference between my Docker image and Puckel's docker image we use mysql as backend rather than postgresql we use rabbitmq as broker rather than postgresql we've implemnet a SystemV style of init script, is user copy anything in /usr/local/airflow/config/init/ of docker contianer , it will be executed before webserver started, this is a perfect place to init airflow variables and connections etc we've implemnet a SystemV style of super-init script, is user copy anything in /usr/local/airflow/config/super-init/ of docker contianer , it will be executed before webserver started , as root, this is a perfect place to init airflow under root user, e.g: fix the hostname and ip mapping issue in /etc/hosts for CeleryExecutor, we have flower enabled to check the task stats all the celery deployment are managed by just one yaml file, I will break them down into details to explain have a dedicated namespace for airflow apiVersion: v1 kind: Namespace metadata: name: airflow have a mysql stateful set with single nodes and a mysql service to act as airflow backend, the mysql data dir is claimed from a PVC, you will need default storageclass in your kubernetes cluster, otherwise this will fail. in normal prodution, it's common to have mysql database served as a dedicated service e.g: AWS RDS. apiVersion: v1 kind: Service metadata: name: mysql namespace: airflow spec: type: NodePort ports: - name: mysql port: 3306 targetPort: 3306 protocol: TCP selector: app: mysql --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: airflow spec: serviceName: &quot;mysql&quot; replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - name: mysql image: email2liyang/mysql-for-airflow:5.7.24 volumeMounts: - name: datadir mountPath: /var/lib/mysql env: - name: MYSQL_ROOT_PASSWORD value: airflow - name: MYSQL_USER value: airflow - name: MYSQL_PASSWORD value: airflow - name: MYSQL_DATABASE value: airflow ports: - containerPort: 3306 # No pre-stop hook is required, a SIGTERM plus some time is all that's # needed for graceful shutdown of a node. terminationGracePeriodSeconds: 60 volumeClaimTemplates: - metadata: name: datadir spec: accessModes: - &quot;ReadWriteOnce&quot; resources: requests: storage: 1Gi have a rabbitmq statefulset to support celery worker scheduling, by the time of (15th Sep 2019) rabbitmq cluster still have issue to be deployed into kubernetes(rabbitmq datadir is based on pod ip, but the pod ip will change every time when pod get restarted, so it's conflict with data persisted in PV), but in our case 1 node of rabbitmq is also fine and we use hostname as the rabbitmq's data dir. apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq namespace: airflow --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader namespace: airflow rules: - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader namespace: airflow subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader --- kind: Service apiVersion: v1 metadata: namespace: airflow name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31672 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30672 selector: app: rabbitmq --- apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config namespace: airflow data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | ## Cluster formation. See https://www.rabbitmq.com/cluster-formation.html to learn more. cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local ## Should RabbitMQ node name be computed from the pod's hostname or IP address? ## IP addresses are not stable, so using [stable] hostnames is recommended when possible. ## Set to &quot;hostname&quot; to use pod hostnames. ## When this value is changed, so should the variable used to set the RABBITMQ_NODENAME ## environment variable. cluster_formation.k8s.address_type = hostname ## How often should node cleanup checks run? cluster_formation.node_cleanup.interval = 30 ## Set to false if automatic removal of unknown/absent nodes ## is desired. This can be dangerous, see ## * https://www.rabbitmq.com/cluster-formation.html#node-health-checks-and-cleanup ## * https://groups.google.com/forum/#!msg/rabbitmq-users/wuOfzEywHXo/k8z_HWIkBgAJ cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal ## See https://www.rabbitmq.com/ha.html#master-migration-data-locality queue_master_locator=min-masters ## See https://www.rabbitmq.com/access-control.html#loopback-users loopback_users.guest = false --- apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq namespace: airflow spec: serviceName: rabbitmq replicas: 1 template: metadata: labels: app: rabbitmq spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: datadir mountPath: /var/lib/rabbitmq ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: [&quot;rabbitmqctl&quot;, &quot;status&quot;] initialDelaySeconds: 60 # See https://www.rabbitmq.com/monitoring.html for monitoring frequency recommendations. periodSeconds: 60 timeoutSeconds: 15 readinessProbe: exec: command: [&quot;rabbitmqctl&quot;, &quot;status&quot;] initialDelaySeconds: 20 periodSeconds: 60 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: RABBITMQ_USE_LONGNAME value: &quot;true&quot; # See a note on cluster_formation.k8s.address_type in the config file section - name: RABBITMQ_NODENAME value: &quot;rabbit@$(MY_POD_NAME).rabbitmq.airflow.svc.cluster.local&quot; - name: K8S_SERVICE_NAME value: &quot;rabbitmq&quot; - name: K8S_HOSTNAME_SUFFIX value: '.rabbitmq.airflow.svc.cluster.local' - name: RABBITMQ_ERLANG_COOKIE value: &quot;mycookie&quot; volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins volumeClaimTemplates: - metadata: name: datadir spec: accessModes: - &quot;ReadWriteOnce&quot; resources: requests: storage: 5Gi have an airflow webserver to provide UI support for airflow,we could manage/ track the dag via UI, kubernetes is check it's liveness and readiness on path /admin/ and port 8080, we could also deploy ingress-nginx with http basic auth to expose the this web UI out side of kubernetes cluster apiVersion: extensions/v1beta1 kind: Deployment metadata: name: airflow-celery-webserver namespace: airflow spec: replicas: 1 selector: matchLabels: app: airflow-celery-webserver template: metadata: labels: name: airflow-celery-webserver app: airflow-celery-webserver spec: containers: - name: airflow-celery-webserver image: docker.io/email2liyang/docker-airflow:1.10.2 imagePullPolicy: Always env: - name: LOAD_EX value: &quot;y&quot; - name: EXECUTOR value: Celery - name: AIRFLOW_MYSQL_DB_HOST value: mysql - name: AIRFLOW_MYSQL_DB_PORT value: &quot;3306&quot; - name: FERNET_KEY value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho= - name: AIRFLOW__CELERY__BROKER_URL value: amqp://guest:guest@rabbitmq:5672 - name: AIRFLOW__CELERY__RESULT_BACKEND value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__EXECUTOR value: CeleryExecutor readinessProbe: httpGet: path: /admin/ port: 8080 initialDelaySeconds: 8 timeoutSeconds: 10 livenessProbe: httpGet: path: /admin/ port: 8080 initialDelaySeconds: 8 timeoutSeconds: 10 ports: - name: webserver containerPort: 8080 command: [&quot;/entrypoint.sh&quot;] args: [&quot;webserver&quot;] --- apiVersion: v1 kind: Service metadata: name: airflow-celery-webserver namespace: airflow spec: type: NodePort ports: - name: webserver port: 8080 targetPort: webserver selector: app: airflow-celery-webserver have an airflow scheduler deployed in kubernetes to schedule tasks into different worker nodes, each cluster could only have 1 scheduler for now, we use ps command to do the liveness and readiness check apiVersion: extensions/v1beta1 kind: Deployment metadata: name: airflow-celery-scheduler namespace: airflow spec: replicas: 1 selector: matchLabels: app: airflow-celery-scheduler template: metadata: labels: name: airflow-celery-scheduler app: airflow-celery-scheduler spec: containers: - name: airflow-celery-scheduler image: docker.io/email2liyang/docker-airflow:1.10.2 imagePullPolicy: Always env: - name: LOAD_EX value: &quot;y&quot; - name: EXECUTOR value: Celery - name: AIRFLOW_MYSQL_DB_HOST value: mysql - name: AIRFLOW_MYSQL_DB_PORT value: &quot;3306&quot; - name: FERNET_KEY value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho= - name: AIRFLOW__CELERY__BROKER_URL value: amqp://guest:guest@rabbitmq:5672 - name: AIRFLOW__CELERY__RESULT_BACKEND value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__EXECUTOR value: CeleryExecutor readinessProbe: exec: command: - /bin/bash - -c - ps -ef | grep scheduler | grep -v &quot;grep&quot; initialDelaySeconds: 8 timeoutSeconds: 10 livenessProbe: exec: command: - /bin/bash - -c - ps -ef | grep scheduler | grep -v &quot;grep&quot; initialDelaySeconds: 8 timeoutSeconds: 10 command: [&quot;/entrypoint.sh&quot;] args: [&quot;scheduler&quot;] have an airflow worker deployed into the kubernetes cluster , we use ps command to do the liveness and readiness check. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: airflow-celery-worker namespace: airflow spec: replicas: 2 selector: matchLabels: app: airflow-celery-worker template: metadata: labels: name: airflow-celery-worker app: airflow-celery-worker spec: containers: - name: airflow-celery-worker image: docker.io/email2liyang/docker-airflow:1.10.2 imagePullPolicy: Always env: - name: LOAD_EX value: &quot;y&quot; - name: EXECUTOR value: Celery - name: AIRFLOW_MYSQL_DB_HOST value: mysql - name: AIRFLOW_MYSQL_DB_PORT value: &quot;3306&quot; - name: FERNET_KEY value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho= - name: AIRFLOW__CELERY__BROKER_URL value: amqp://guest:guest@rabbitmq:5672 - name: AIRFLOW__CELERY__RESULT_BACKEND value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__EXECUTOR value: CeleryExecutor readinessProbe: exec: command: - /bin/bash - -c - ps -ef | grep celeryd | grep -v &quot;grep&quot; initialDelaySeconds: 8 timeoutSeconds: 10 livenessProbe: exec: command: - /bin/bash - -c - ps -ef | grep celeryd | grep -v &quot;grep&quot; initialDelaySeconds: 8 timeoutSeconds: 10 command: [&quot;/entrypoint.sh&quot;] args: [&quot;worker&quot;] optionally, have an airflow celery flower deployed into the cluster to monitor celery work's health, we use path / and port 5555 to do the readiness and liveness check. with celery workers managed by kubernetes, you may see some celery workers come and go, this is natture in kubernetes world. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: airflow-celery-flower namespace: airflow spec: replicas: 1 selector: matchLabels: app: airflow-celery-flower template: metadata: labels: name: airflow-celery-flower app: airflow-celery-flower spec: containers: - name: airflow-celery-flower image: docker.io/email2liyang/docker-airflow:1.10.2 imagePullPolicy: Always env: - name: LOAD_EX value: &quot;y&quot; - name: EXECUTOR value: Celery - name: AIRFLOW_MYSQL_DB_HOST value: mysql - name: AIRFLOW_MYSQL_DB_PORT value: &quot;3306&quot; - name: FERNET_KEY value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho= - name: AIRFLOW__CELERY__BROKER_URL value: amqp://guest:guest@rabbitmq:5672 - name: AIRFLOW__CELERY__RESULT_BACKEND value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow - name: AIRFLOW__CORE__EXECUTOR value: CeleryExecutor readinessProbe: httpGet: path: / port: 5555 initialDelaySeconds: 8 timeoutSeconds: 10 livenessProbe: httpGet: path: / port: 5555 initialDelaySeconds: 8 timeoutSeconds: 10 ports: - name: flower containerPort: 5555 command: [&quot;/entrypoint.sh&quot;] args: [&quot;flower&quot;] --- apiVersion: v1 kind: Service metadata: name: airflow-celery-flower namespace: airflow spec: type: NodePort ports: - name: flower port: 5555 targetPort: flower selector: app: airflow-celery-flower now we could achieve the goal to manage the celery worker by just one command, if we want 20 celery workers, we could simply execute command kubectl scale --replicas=20 deployment airflow-celery-worker -n airflow , then you will get 20 celery works in airflow cluster. I will discuss more on how to organize airflow project in next article.","path":"infra/kube/deploy-apache-airflow-cluster-into-kubernetes.html","title":"Deploy Apache Airflow Cluster into Kubernetes"},{"content":"When we create EKS cluster based on offical developer guide or terraform module, we will get default Auto Scaling Groups(ASG) to manage the ec2 worker nodes in EKS cluster, we could adjust the instance count in the ASG to adjust the number of ec2 worker nodes in the EKS cluster, but when ASG is created we could not change the instance type of worker nodes's availability zone(AZ). I have a scenario that I will need to provision very large ec2 instances for a specific job and terminiate the giant ec2 instances after job complete. with the power of EKS and ASG, I could easily achieve the goal. Auto Scaling Launch Configuration When an EKS cluster is created, there will be at least 1 Auto Scaling Launch Configuration created for the cluster to manage the ec2 worker nodes, we could re-use this Launch Configuration by copy and modify pattern. when copy an exist Launch Configuration we could re-use most of the key information which already worked for EKS cluster and just change the Instance Type to what we want. some key configurations are AMI ID: AWS is using optimised AMI for different verison of kubernetes and each region has its own AMI,you could find the right AMI you could use in this matrix Iam Instance Profile: use to controle aws permission of the worker nodes, normally just re-use it Key name: the key user could use to connect to ec2 instance, normally just re-use it and it's very rare to connect ec2 instnace in EKS env. Instance Type: the ec2 instance type you want, normally we need to change the value to meet our needs Auto Scaling Groups After the Auto Scaling Launch Configuration is in place, we could create Auto Scaling Groups based on the Auto Scaling Launch Configuration, there are some config fields we need to pay attention to Desired Capacity: how many ec2 instance you want in this ASG Min and Max: to control the ec2 instances limit , normally we set Min to 0 and Max to the max ec2 instance number we want. Availability Zone(s): which AZ the ec2 instance need to be provision to, before kubernetes 1.12 , Topology-Aware Volume Provisioning is not supported , and AWS EBS could not be mounted across AZ, so be careful when set multiple AZs in the ASG when you need to provision PV and mount to pod. Subnets: this connect to Availability Zone(s) as well , determine which subnets the ec2 need to be provision to Tags: one special tag need to set on the ASG, the tag format is kubernetes.io/cluster/${clusterName}=owned, and must set Tag New Instance to yes, so that when new ec2 instances are provisioned in this ASG, it will have a tag kubernetes.io/cluster/${clusterName}=owned by default and EKS will use this tag to discover new ec2 work nodes in the cluster Script to provision ec2 nodes to join EKS we could use some simple shell command to provision the ec2 nodes in ASG and let the join EKS use this command to determine how many ec2 instances already in the ASG aws --profile=vipmind-prod \\ autoscaling describe-auto-scaling-groups \\ --auto-scaling-group-name ${asg_group_name} \\ | jq '.AutoScalingGroups | .[0] | .Instances | .[] | .InstanceId' \\ | grep 'i-' \\ | wc -l \\ | sed 's/ //g' use this command to provisoin new ec2 instances in ASG aws --profile=vipmind-prod \\ autoscaling set-desired-capacity --desired-capacity ${desired_worker_node_count} \\ --auto-scaling-group-name ${asg_group_name} \\ --no-honor-cooldown use this command to determine how many nodes are avaialble in EKS cluster kubectl --kubeconfig=${HOME}/.kube/kubeconfig_vipmind-prod \\ get nodes \\ | grep -v 'NAME' \\ | wc -l \\ | sed 's/ //g' wait all the worker nodes to be ready not_ready_worker_node_count=1 while [[ ${not_ready_worker_node_count} -gt 0 ]]; do echo &quot;${not_ready_worker_node_count} not ready, sleep 5s and check again&quot; sleep 5 not_ready_worker_node_count=`kubectl \\ --kubeconfig=${HOME}/.kube/kubeconfig_vipmind-prod \\ get nodes \\ | grep 'NotReady' \\ | wc -l \\ | sed 's/ //g'` done we could also label new created nodes with some tags, so that we could easily determine the ec2 worker nodes usage node_names=`aws --profile=vipmind-prod \\ ec2 describe-instances \\ --instance-ids ${instance_ids} \\ --query 'Reservations[*].Instances[*].[PrivateDnsName]' \\ | jq '.[] | .[] | .[]' \\ | sed 's/&quot;//g'` kubectl --kubeconfig=${HOME}/.kube/kubeconfig_vipmind-prod label nodes ${node_names} vipmind/usage=${node_tag} kubectl --kubeconfig=${HOME}/.kube/kubeconfig_vipmind-prod label nodes ${node_names} aws/asg-group-name=${asg_group_name} Script to destroy ec2 nodes in EKS when the specific job complete, we could terminiated the ec2 work nodes in the cluster with one command aws --profile=vipmind-prod \\ autoscaling set-desired-capacity --desired-capacity 0 \\ --auto-scaling-group-name ${asg_group_name} \\ --no-honor-cooldown if for some reason, user do not want to terminate all the nodes in a ASG, user could use command like below to destroy node in ASG, see ref aws --profile=vipmind-prod \\ autoscaling terminate-instance-in-auto-scaling-group \\ --instance-id ${ec2_instance_id} \\ --should-decrement-desired-capacity ","path":"infra/kube/managing-eks-worker-nodes-dynamically-by-aws-auto-scaling-groups.html","title":"Managing EKS Worker nodes dynamically by AWS Auto Scaling Groups "},{"content":"AWS EKS has document on how to connect IAM and EKS , but the documentation is too generic, what should I do if I want to grant some user read only access across the cluster? what should I do if I want to grant full permissions on a specific namespace to some user ? there is no documentation for that, in this article I'm going to explain how to achieve this. When an EKS cluster is created, the user who create the EKS cluster becomes the cluster administrator by default (the user is automatically granted system:masters permissions in the cluster's RBAC configuration), the problem with system:masters permissions is that this permission is too power full, user may not want to grant everyone this permission. we could grant grant specific permission to specific iam user by create Kubernetes RBAC resources and edit aws-auth configmap grant read only access to someone across the cluster to grant permission across cluster level, ClusterRole and ClusterRoleBinding are needed, the ClusterRole will define the permission in EKS cluster, the ClusterRoleBinding will bind this ClusterRole to a specific group named vipmind:cluster-read-only apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; labels: name: cluster-read-only namespace: default rules: - apiGroups: - &quot;&quot; resources: [\"*\"] verbs: - get - list - watch - apiGroups: - extensions resources: [\"*\"] verbs: - get - list - watch - apiGroups: - apps resources: [\"*\"] verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-read-only roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-read-only subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: vipmind:cluster-read-only next we could edit aws-auth configmap to map the permission to IAM user, like below, in the mapUsers section, the userarn could map to IAM user's arn, the groups value could be vipmind:cluster-read-only, so that user ivanli could have read only access across the cluster, the action ivanli could do is defined in ClusterRole cluster-read-only apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: arn:aws:iam::xxxxx:role/xxxx-eks-worker-nodes-stackxx username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes mapUsers: | - userarn: arn:aws:iam::xxxx:user/ivanli username: ivanli groups: - vipmind:cluster-read-only grant full permissions on a specific namespace to some user to grant permission in namespace level, Role and RoleBinding are needed, the Role will define the permission in namespace flow, the RoleBinding will bind this Role to a specific group named vipmind:flow-admin apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; labels: name: flow-admin namespace: flow rules: - apiGroups: - \"*\" resources: - \"*\" verbs: - \"*\" --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: flow-admin namespace: flow roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: flow-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: vipmind:flow-admin next we could edit aws-auth configmap to map the permission to IAM user, like below, in the mapUsers section, the userarn could map to IAM user's arn, the groups value could be vipmind:flow-admin, so that user ivanli could have full access in namespace flow, the action ivanli could do is defined in Role flow-admin apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: arn:aws:iam::xxxxx:role/xxxx-eks-worker-nodes-stackxx username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes mapUsers: | - userarn: arn:aws:iam::xxxx:user/ivanli username: ivanli groups: - vipmind:cluster-read-only - vipmind:flow-admin Now the IAM user ivanli could have read only access across the whole EKS cluster and have full permission on namespace flow, with this type of config, the EKS administrator could manage the EKS permission in an efficient way.","path":"infra/kube/managing-eks-permissions-with-kubernetes-rbac-and-aws-iam.html","title":"Managing EKS permissions with Kubernetes RBAC and AWS IAM"},{"content":"Kubernetes has lots of Ingress Controller implementations,I've been using ingress-nginx for a long time, this ingress controller works great. In AWS EKS environment, it create a classic ELB to work in layer 4 , all the incomming request will be forward to nginx and do layer 7 dispatch. such that if you want to provide https support, you have manage the SSL/TSL certificate by your own. We are fully operated on AWS infrastructure and AWS Certificate Manager could help us to manage the SSL/TSL certificate in an efficient way. but the problem is that the public certificate would never go out of AWS infrastructure, and you could never download the certificated and load it for ingress-nginx, luckly, aws has it's own ingress controller implementation AWS ALB Ingress Controller. it will create an Application Load Balancer(ALB) and work on layer 7 to forward request to the kubernetes service directly, the ALB could also work with AWS Certificate Manager out of box to support HTTPS. there are already article to describe how to deploy ALB ingress controller into EKS, you could read the details if you are interested in, I will not repeat them. I want to point out some tips when I'm actually working on it in the alb-ingress-controller.yaml there are 3 parameter must be proviced --cluster-name=my-eks-cluster-name : ALB will use this to identify which EKS this ALB should connect to --aws-vpc-id=vpc-my-vpc-id: used to detect which VPC the ALB should be deployed to --aws-region=xxx: used to determine where the ALB should be created to after the 2048 games application get deployed, I want to serve the application in my own domain name over https, I could use use AWS Certificate Manager to help me to manage SSL certificate in my ACM, I have a domain named vipmind.me registered, ACM will provide me an ARN e.g: arn:aws:acm:us-east-1:myaccount:certificate/xxxxxxxxxxxxxxxxxxx in my ingress.yaml file, I could use annotation alb.ingress.kubernetes.io/certificate-arn to declare my ACM ARN so that ALB could use the SSL/TSL provided by ACM annotation kubernetes.io/ingress.class:alb is important when you have multiple ingress controllers in one EKS, this will indicate that ALB ingress controller would react on this ingress config annotation alb.ingress.kubernetes.io/listen-ports: '[{&quot;HTTPS&quot;:443}]' will force ALB to create listen on https port annotatiton alb.ingress.kubernetes.io/scheme: internet-facing will make sure this ALB is exposed to internet, it will also need the EKS public subnet is tagged with kubernetes.io/role/elb=1 The full yaml config file could looks like below apiVersion: extensions/v1beta1 kind: Ingress metadata: name: &quot;2048-ingress&quot; namespace: &quot;2048-game&quot; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/listen-ports: '[{&quot;HTTPS&quot;:443}]' # ACM certificate ARN for your SSL domain alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:myaccount:certificate/xxxxxxxxxxxxxxxxxxx labels: app: 2048-ingress spec: rules: - host: 2048.vipmind.me http: paths: - path: /* backend: serviceName: &quot;service-2048&quot; servicePort: 80 After the ingress is deployed, an ALB will be createtd in AWS, it may take about 5 mins to provision the ALB. then we could grab the dns name for this ALB, let's assume it's ffggsss-2048game-2048ingr-6fa0-xxxxxxx.us-east-1.elb.amazonaws.com, open Route53's domain config, add a new entry 2048.vipmind.me set the CNAME value to ffggsss-2048game-2048ingr-6fa0-xxxxxxx.us-east-1.elb.amazonaws.com. I could access the 2048 games via url https://2024.vipmind.me over https and aws will take care of all the SSL complexity for me There are still some limitation in ALB the ingress could not reference service across namespaces, so the ingress have to stay same namespace with the service echo ingress yaml file will create a new ALB, this some times is confusing and not expected , because AWS also have limitaiton on ALB number per account per region. if you have lots of service to expose, you will have problem on ALB number limitation under your account keep eye on this github issue to track this limitation https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/298 ","path":"infra/kube/expose-webapplicant-with-aws-alb-ingress-controller-and-aws-certifacate-manager-over-https.html","title":"Expose WebApplicant with AWS ALB Ingress Controller and AWS Certifacate Manager Over HTTPS"},{"content":" Google Guava is a powerful tool, almost every java project is using it as a utility lib. Guava loading cache is one of the most frequently used component. one common mistake about the loading cache is that it's very easy to misunderstand how the cache is get evicted. there are 3 types of evication guava support Size-based Eviction Timed Eviction Referencee-based Eviction for Timed Eviction, Guava provide api expireAfterAccess(long, TimeUnit) and expireAfterWrite(long, TimeUnit) to let developer to speicify when the item in the cache will get evicted. when you have heavy write and read on the cache object , there is no problem at all, but if you have rare write but heavy read on cache object, this will become a problem, here is how Guava Cache is documented Caches built with CacheBuilder do not perform cleanup and evict values &quot;automatically,&quot; or instantly after a value expires, or anything of the sort. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare. The reason for this is as follows: if we wanted to perform Cache maintenance continuously, we would need to create a thread, and its operations would be competing with user operations for shared locks. Additionally, some environments restrict the creation of threads, which would make CacheBuilder unusable in that environment. Instead, we put the choice in your hands. If your cache is high-throughput, then you don't have to worry about performing cache maintenance to clean up expired entries and the like. If your cache does writes only rarely and you don't want cleanup to block cache reads, you may wish to create your own maintenance thread that calls Cache.cleanUp() at regular intervals. In my use case, I have rare write but very heavy read, so I have to create my own thread to do the Guava cache clean up. Below is the snip how use RxJava to clean up the Guava cache periodically //declare the loading cache private LoadingCache&lt;String, Integer&gt; versionCache = CacheBuilder.newBuilder() .expireAfterAccess(30, TimeUnit.MINUTES) .build(new CacheLoader&lt;String, Integer&gt;() { @SuppressWarnings(&quot;Duplicates&quot;) @Override public Integer load(String key) throws Exception { Integer currentVersion = xxx.getConfigValue(key); return currentVersion; } }); @Inject public MyService() { Observable .interval(30, 30, TimeUnit.MINUTES) .doOnNext(timeInterval -&gt; versionCache.cleanUp()) .onErrorReturn(t -&gt; { log.warn(&quot; error on clean up my cache&quot;, t); //the return value has no business value, just make sure //the cache can be cleaned up every 30 minutes return 1L; }) .retry() .subscribe(); } ","path":"programing/java/google-guava-cache-cleanup-tip.html","title":"Google Guava Cache Cleanup Tip"},{"content":"Today I meet a very high RDS server load in production , the CPU load increased to 100%, the application which rely on this database become very slow. here is how I find the bottelneck and fix the problem. use command SHOW FULL PROCESSLIST \\G to show what's current running in mysql and I got lots of queries like below *************************** 17. row *************************** Id: 1326504 User: xx Host: 10.x.x.x:35724 db: xx Command: Query Time: 1 State: Sending data Info: select serviceadd0_.id as id1_25_, serviceadd0_.address_1 as address_2_25_ from service_address serviceadd0_ where serviceadd0_.str_hash='802D017ABA5D5D9195807CB0E8FD6362' then use command SHOW ENGINE INNODB STATUS \\G to show the state of the InnoDB storage engine, there are lots of output from this command , one interesting out looks like below -------------- ROW OPERATIONS -------------- 0 queries inside InnoDB, 0 queries in queue 5 read views open inside InnoDB Process ID=5498, Main thread ID=47384635647744, state: sleeping Number of rows inserted 376417855, updated 54788226, deleted 2218100, read 2056241102537 5.62 inserts/s, 4.12 updates/s, 0.00 deletes/s, 1610388.95 reads/s the reads per second is too large and unexpected, this indicate that maybe the query does not use any index and doing a table scan instead , then it will cause lots of db reads, to prove this, I could see waht's the query plan for the given query by command explain select serviceadd0_.id as id1_25_, serviceadd0_.address_1 as address_2_25_ from service_address serviceadd0_ where serviceadd0_.str_hash='802D017ABA5D5D9195807CB0E8FD6362' then I could get result like below id|select_type|table |partitions|type|possible_keys|key|key_len|ref|rows |filtered|Extra | --|-----------|------------|----------|----|-------------|---|-------|---|-------|--------|-----------| 1|SIMPLE |serviceadd0_| |ALL | | | | |1460077| 10|Using where| the out put indicate that the query is doing a table scan and query 1460077 rows per query, this will generate a high io read in the db server. to fix this I create a index on field str_hash, then check the query plan again, it impoved a lot id|select_type|table |partitions|type|possible_keys |key |key_len|ref |rows|filtered|Extra| --|-----------|------------|----------|----|--------------------|--------------------|-------|-----|----|--------|-----| 1|SIMPLE |serviceadd0_| |ref |idx_str_hash_country|idx_str_hash_country|32 |const| 1| 100| | After the index is in place, the server load dropped a lot , the CPU load had dropped from 100% to 10%, after fix the production issue, it's time to check why this happen, finally I find out that we do have an index for field str_hash, but it's a composite index and field str_hash is placed in 2nd field. the old index is defiend on fields country and str_hash, maybe the business logic has changed and do not have country in the search criteria , so mysql could not fully use the old index and use table scan instead. when I run SHOW ENGINE INNODB STATUS \\G again , the reads per seconds also get dropped -------------- ROW OPERATIONS -------------- 0 queries inside InnoDB, 0 queries in queue 0 read views open inside InnoDB Process ID=5498, Main thread ID=47384635647744, state: sleeping Number of rows inserted 376698149, updated 55015089, deleted 2218102, read 2060159885967 3.28 inserts/s, 0.00 updates/s, 0.00 deletes/s, 665.47 reads/s ","path":"infra/aws/a-journey-of-fixing-100-percent-cpu-load-on-aws-mysql-rds.html","title":"A journey of fixing 100 percent CPU load on AWS MySQL RDS"},{"content":"I have a data flow to insert about 100M records into mongoDB, I'm using Beam to run on Flink cluster to deal with the the concurrency, for this job I've enable 30 task managers in the beam cluster. I'm expecting this process to complete within 24 hours based on historical experience. but when I start the job and monitor the process, it does not behave as expected. the mongoDB insert is extramly slow and the mongoDB cluster's load has increased dramatically. the job need to take about 1 year to complete, that's totally not acceptable.Finally I find out that the index fo the target collection in mongoDB is missing, which cause every db insert result in a disk scan, this is getting worse and worse when you inserted more and more data. because everytime when I do a db insert, the mondoDB is need to scan exist data in the collection to check if there is exist record to peform update otherwise perform a fresh insert. after I create the index for the target collection, the mongoDB performance boost up quickly and the server load dropped off dramatically, currently the job is running well and it will be complete within 12 hours. MongoDB has veriouse types of indexes， I'm using multikey indexes in my case because I will need to search by multiple key first to update before I do an insert. there are many good reason to do such upsert types of db operation, one good benifit is that it could make the db operation atomic and idempotent. atomic and idempotent are very important in big data processing system, this could enable the flow to be able to re-run without any problem. There is a framework named morphia to help us to do this work very easily. we could create the JPA like entity class to manage the mongoDB index in the code. @Entity(noClassnameStored = true) @Indexes( { @Index(fields = { @Field(&quot;id&quot;), @Field(&quot;key1&quot;), @Field(&quot;key2&quot;), @Field(&quot;key3&quot;) }, options = @IndexOptions(unique = true)), @Index(fields = { @Field(&quot;key1&quot;), @Field(&quot;key2&quot;), @Field(&quot;key3&quot;), @Field(value = &quot;v1&quot;, type = IndexType.DESC) }), @Index(fields = { @Field(&quot;key1&quot;), @Field(&quot;key2&quot;), @Field(&quot;key3&quot;), @Field(value = &quot;v2&quot;, type = IndexType.DESC) }) @Index(fields = { @Field(value = &quot;lastUpdated&quot;, type = IndexType.DESC) }) } ) public class XXXEntity{} in XXXEntity, I'm defining 4 indexes , the 1st index is used for upsert lookup the 2nd and 3rd indexes are used for online query and it could order by v1 or v2, if I only need key1, key2 and key3 and v1 or v2, the search performance will be even better, because it's a covered query. to perform an atomic update in mongoDB, we could do like below, the last parameter means createIfMissing, when we set it to true, morphia will insert a new record if the given query could not find any match UpdateOperations&lt;XXXEntity&gt; updateOperations = datastore .createUpdateOperations(XXXEntity.class) .setOnInsert(&quot;id&quot;, xxxPojo.getId()) .setOnInsert(&quot;key1&quot;, xxxPojo.getKey1()) .setOnInsert(&quot;key2&quot;, xxxPojo.getKey2()) .setOnInsert(&quot;key3&quot;, xxxPojo.getKey3()) .set(&quot;v1&quot;, xxxPojo.getV1()) .set(&quot;v2&quot;, xxxPojo.getV2()) // set update time based on write (regardless of what caller requested) .set(&quot;lastUpdated&quot;, LocalDateTime.now(Clock.systemUTC())); Query&lt;XXXEntity&gt; query = datastore .createQuery(&quot;collectionName&quot;, XXXSumEntity.class) .field(&quot;id&quot;).equal(xxxPojo.getId()) .field(&quot;key1&quot;).equal(xxxPojo.getKey1()) .field(&quot;key2&quot;).equal(xxxPojo.getKey2()) .field(&quot;key3&quot;).equal(xxxPojo.getKey3()); UpdateResults results = datastore.update(query, updateOperations, true); because we have defined the index on &quot;id,key1,key2,key3&quot;, the mongoDB do not need to do collection scan for the query, it will use the index to find out the match. Before the index is created, I could only insert 5 to 6 record per second, most of the time is spend on the collection scan and the IO load is very heavy, after the index is created, I could insert 2263 records per seconds.","path":"data/datastore/index-is-key-to-mongo-performance.html","title":"Index is Key to Mongo Performance"},{"content":"Document Node is built by my old friend, it's a perfect tool to build a website in markdown grammer, the idea of Document Node is to let the user to use very less time to config the website and focus on the writting part, it support markdown grammer out of box and could export the content in both html and pdf format, if you want to learn more about Document Node, please reference their official website Github Pages is awesome for static website hosting , it support customised domains over SSL and it has build in CDN support, the most important part is that it's free. Build personal blog with Document Node I feel like these 2 tools could match perfectly and could help me to build my personal blog very efficiently and with very minimum cost. here is the steps I use to build my personal blog. Download Document Node and install into my Mac Open Document Node and crate a new project named &quot;vipmind.me&quot; Create a git repository named vipmind.me under my github account to hold my blog raw content Check in folder &quot;vipmind.me&quot; into git repository vipmind.me Create another git repository named email2liyang.github.io as my Github pages project Do git clone into my local folder named &quot;email2liyang.github.io&quot; Open Document Node to create a new filder named &quot;life&quot; Create a new markdown file named &quot;Build personal blog with Document Node and Github Pages.md&quot; Focus on write content in the markdown file and I could also have a live preview in Document Node when I am writing. After Complete the writing, click &quot;Export Project&quot; button in Document Node and let Document Node to export the website content into folder &quot;email2liyang.github.io&quot; Then commit and push everything under folder &quot;email2liyang.github.io&quot; I could access my peresonal website from https://email2liyang.github.io Setup Github Pages customised domains over SSL It's good to have a customised domain name for my personal blog so that I purcharse vipmind.me as my domainn name from Godaddy, it just cost me $5 for the first year, it's a very good deal for my case, then I could use this domain to setup my personal blog with the following steps Go to godaddy's domain management console Click the link undere &quot;vipmind.me&quot;, in section &quot;Additional Settings&quot; click link &quot;Manage DNS&quot; In the Records table, add a new CNAME &quot;www.vipmind.me&quot; to &quot;email2liyang.github.io&quot; Go to github repository email2liyang.github.io's setting page Scroll to &quot;Github Pages&quot; section and input &quot;www.vipmind.me&quot; in the Custom domain field Click &quot;Enforce HTTPS&quot; check box starting from 2016, Github is paring with Let's Encrypt to generate the SSL certificate automatically the SSL certificate may take about 5 mins to be in place After all the settings above, I could have my personal blog up and running !","path":"life/build-personal-blog-with-document-node-and-github-pages.html","title":"Build personal blog with Document Node and Github Pages"},{"content":"It's very confusing for the beginner to understand how python works with the dependencies and there are lots of tools avaialble in the market try to resolve this problem, for example: pyenv, virtualenv, pyenv-virtualenv, virtualenvwrapper, pyenv-virtualenvwrapper, pipenv, you could check the differnece from stackoverflow There are 3 main problems these tools are trying to resolve. how to choose python version for your project normally differnet OS may have different build in python version and it's always avalialbe in your path but for some project , the developer want to choose a different version of python. how to manage python dependencies for your project python is managing it's dependencies globally by default. one project may want to use version 1 of lib A, but other project may want to use version 2 of lib A, if we still manage the dependencies globally, it will get conflict python tools try to create a virtual env for each specific project, so that each project could manage it's own dependencies. lots of tools , like virtualenv, pyenv-virtualenv, virtualenvwrapper, pyenv-virtualenvwrapper they are all trying to resolve this problem, starting from python 3.3 there is a build in module named venv to help developer to create virtual env too. developer could execute command python3 -m venv to do this. how install 3rd party dependencies for your project pip has become the de facto standard python dependency management tool. pip could use requirements.txt to manage it's dependencies but it can be problematic , that's why pipenv was invented After a long journey with all of these tools , I finally select pyenv and pipenv to manage my python developmenet env. I could use pyenv to manage different version of python I could use and install any python version I want into my system. To list all available python versions to install, issue command pyenv install -l, it will give me a long list of avaialble python versions Available versions: 2.1.3 2.2.3 2.3.7 ... to setup a python version for given project, just go to project folder and exec command pyenv local 3.6.8, this will set my project to use python version 3.6.8 by generating a file named .python-version Next I could use pipenv to setup my virutal env and manage python dependencies. I have a system env setup as PYENV_VIRTUALENV_INIT=1 in my ${HOME}/.bash_profile so that pipenv will create venvin folder .venv insidee my project so that my project is just self contained. then I could issue command pipenv install grpcio to install grpc support for python. this command will generate 2 files Pipfile and Pipfile.lock, these 2 files are used to manage python dependencies and expected to be managed by version control system like git. pipenv could also generate requirements.txt to integrate with pip by command pipenv lock -r &gt; requirements.txt After installed all the python dependencies , we could issue command pipenv shell to activate current python env and it will load dependencies from pipenv managed virtual env There is one drawback when you use pipenv, pipenv is trying to analyse the python dependency and there is a concept named locking in pipenv, for now the locking process may be slow for some python dependency","path":"programing/python/set-up-python-development-env-with-pyenv-and-pipenv.html","title":"Set up Python Development Env with pyenv and pipenv"}]