<!DOCTYPE html>
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-147718386-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-147718386-1');
</script>


  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <title>Mind - Deploy Apache Airflow Cluster into Kubernetes</title>
  
    
  
  <meta name="description" content="Apache airflow is a powerful tool in data processing world, below is the official airflow introduction Airflow is a platform to programmatically author, schedule and monitor workflows. Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of wo">
  
  
  
  <link rel="shortcut icon" href="../../images/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="../../images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../images/favicon/favicon-16x16.png">
  <link rel="manifest" href="../../images/favicon/site.webmanifest">
  <link rel="mask-icon" href="../../images/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  
  <link href='../../styles/main.min.css?v=1622967210769' rel='stylesheet' type='text/css'>
  
  <style></style>

  
<script>
  var doc_layout_plugin = {
    enabled: true
  };
</script>
</head>
<body>


<script async type="text/javascript" language="javascript" src="../../plugins/plugins.nocache.js?v=1622967210769"></script>
<iframe src="javascript:''" id="__gwt_historyFrame"
        style="position:absolute;width:0;height:0;border:0" tabIndex="-1"></iframe>
<!--[if lt IE 8]>
    <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
<![endif]-->

<div class="nav b5">
  <div class="nav__content max-width-md">
    <div class="nav__content__left">
      <div class="nav__header">
        <a class="logo" href="../../index.html" title="Home">
          <img class="logo__img" src="../../images/logo.png" alt="VIP">
          <span class="logo__name">Mind</span><span class="superscript">me</span>
        </a>
        <label id="menu-icon" class="nav__menu-toggle" for="menu-toggle-1">
          <svg class="nav__menu-icon" viewBox="0 0 448 512"><rect y="398" width="448" height="36"/><rect y="78" width="448" height="36"/><polygon points="6 238 0 238 0 244 0 256 0 274 448 274 448 256 448 244 448 238 442 238 6 238"/></svg>
        </label>
      </div>
      <input id="menu-toggle-1" class="hide" type="checkbox">
      <div id="nav-menus-main" class="nav__menus">
        <a href="../../programing.html">Programing</a>
        <a href="../../infra.html">Infrastructure</a>
        <a href="../../data.html">Data Processing</a>
        <a href="../../life.html">Life is like chocolate</a>
      </div>
    </div>
    <input id="menu-toggle-2" class="hide" type="checkbox">
    <div id="nav-menus-right" class="nav__content__right">
      <a href="../../about.html">About</a>
    </div>
  </div>
</div><div id="crumb-bar" class="cbb b6">
  <div class="cbb__content max-width-md">
    <div class="left-part">
      <div class="breadcrumb-item">
        <a href="../..//">Home</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>
      <div class="breadcrumb-item">
        <a href="../.././toc.html">Table of Contents</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>

      <div class="breadcrumb-item">
        <a href="../../infra.html">Infra</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>
      <div class="breadcrumb-item">
        <a href="../kube.html">Kube</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>
      <div class="breadcrumb-item">
        <span>Deploy Apache Airflow Cluster into Kubernetes</span>
      </div>

    </div>
    <div class="right-part">
      <div class="search-field">
        <img class="icon-search" src="../../images/icon-search-gray.svg" alt="icon search gray">
        <input id="breadcrumb-search-box" class="search-box" type="search" placeholder="Search... (double â‡§)">
      </div>
      <img id="back-to-top" src="../../images/icon-top-darkgray.svg" title="Back to Top" alt="icon top darkgray">
    </div>
  </div>
</div><div class="c-title b7">
  <div class="c-title__content max-width-md">
    <h1>Deploy Apache Airflow Cluster into Kubernetes</h1>
    <div class="meta">
      <img class="avatar" src="../../images/default-avatar.jpg" alt="Ivan" title="Ivan">
      <a class="author">Ivan</a>
      <span class="meta-divider">/</span>
      <span class="updated">15 Sep 2019 16:47:54</span>
    </div>
  </div>
</div><div class="c-article b8">
  <div class="c-article__content max-width-sm">
    <div id="article-body" class="article-body">
      <p><a href="https://airflow.apache.org/">Apache airflow</a> is a powerful tool in data processing world, below is the official airflow introduction</p>
<blockquote>
<p>Airflow is a platform to programmatically author, schedule and monitor workflows.<br />
Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.</p>
</blockquote>
<p><a href="https://airflow.apache.org/">Apache airflow</a> support <a href="http://www.celeryproject.org">Celery</a> as its task queue to distribute tasks across different worker nodes.  it will be great if we could deploy apache airflow into kubernetes cluster and let kubernetes to manage the celery cluster with simple command <code>kubectl scale --replicas=N airflow-worker</code>, that's why my project <a href="https://github.com/email2liyang/airflow-kubernetes">airflow-kubernetes</a> was born.</p>
<p><figure><img src="../../images/airflow-on-kube.png" alt="airflow-on-kube"></figure></p>
<p>I'm re-using an popular <a href="https://github.com/puckel/docker-airflow">airflow docker image</a>, but modify it to support deploy airflow into kubernetes with both LocalExecutor and CeleryExecutor. the LocalExecutor is useful for local development, the CeleryExecutor is useful for production deployment with as many celery works as you want in kubernetes cluster. there are some difference between my Docker image and Puckel's docker image</p>
<ul>
<li>we use mysql as backend rather than postgresql</li>
<li>we use rabbitmq as broker rather than postgresql</li>
<li>we've implemnet a SystemV style of init script, is user copy anything in /usr/local/airflow/config/init/ of docker contianer , it will be executed before webserver started, this is a perfect place to init airflow variables and connections etc</li>
<li>we've implemnet a SystemV style of super-init script, is user copy anything in /usr/local/airflow/config/super-init/ of docker contianer , it will be executed before webserver started , as root, this is a perfect place to init airflow under root user, e.g: fix the hostname and ip mapping issue in /etc/hosts</li>
<li>for CeleryExecutor, we have flower enabled to check the task stats</li>
</ul>
<p>all the celery deployment are managed by just <a href="https://github.com/email2liyang/airflow-kubernetes/blob/master/KubernetesCeleryExecutor.yaml">one yaml file</a>, I will break them down into details to explain</p>
<ol>
<li>have a dedicated namespace for airflow</li>
</ol>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: airflow
</code></pre>
<ol start="2">
<li>have a mysql stateful set with single nodes and a mysql service to act as airflow backend, the mysql data dir is claimed from a PVC, you will need default storageclass in your kubernetes cluster, otherwise this will fail. in normal prodution, it's common to have mysql database served as a dedicated service e.g: AWS RDS.</li>
</ol>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: airflow
spec:
  type: NodePort
  ports:
  - name: mysql
    port: 3306
    targetPort: 3306
    protocol: TCP
  selector:
    app: mysql

---

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: airflow
spec:
  serviceName: &quot;mysql&quot;
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: email2liyang/mysql-for-airflow:5.7.24
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: airflow
        - name: MYSQL_USER
          value: airflow
        - name: MYSQL_PASSWORD
          value: airflow
        - name: MYSQL_DATABASE
          value: airflow
        ports:
        - containerPort: 3306
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
      - &quot;ReadWriteOnce&quot;
      resources:
        requests:
          storage: 1Gi
</code></pre>
<ol start="3">
<li>have a rabbitmq statefulset to support celery worker scheduling, by the time of (15th Sep 2019) rabbitmq cluster still have issue to be deployed into kubernetes(rabbitmq datadir is based on pod ip, but the pod ip will change every time when pod get restarted, so it's conflict with data persisted in PV), but in our case 1 node of rabbitmq is also fine and we use hostname as the rabbitmq's data dir.</li>
</ol>
<pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  name: rabbitmq 
  namespace: airflow
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: endpoint-reader
  namespace: airflow
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;endpoints&quot;]
  verbs: [&quot;get&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: endpoint-reader
  namespace: airflow
subjects:
- kind: ServiceAccount
  name: rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: endpoint-reader
---
kind: Service
apiVersion: v1
metadata:
  namespace: airflow
  name: rabbitmq
  labels:
    app: rabbitmq
    type: LoadBalancer  
spec:
  type: NodePort
  ports:
   - name: http
     protocol: TCP
     port: 15672
     targetPort: 15672
     nodePort: 31672
   - name: amqp
     protocol: TCP
     port: 5672
     targetPort: 5672
     nodePort: 30672
  selector:
    app: rabbitmq
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-config
  namespace: airflow
data:
  enabled_plugins: |
      [rabbitmq_management,rabbitmq_peer_discovery_k8s].
  rabbitmq.conf: |
      ## Cluster formation. See https://www.rabbitmq.com/cluster-formation.html to learn more.
      cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
      cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
      ## Should RabbitMQ node name be computed from the pod's hostname or IP address?
      ## IP addresses are not stable, so using [stable] hostnames is recommended when possible.
      ## Set to &quot;hostname&quot; to use pod hostnames.
      ## When this value is changed, so should the variable used to set the RABBITMQ_NODENAME
      ## environment variable.
      cluster_formation.k8s.address_type = hostname
      ## How often should node cleanup checks run?
      cluster_formation.node_cleanup.interval = 30
      ## Set to false if automatic removal of unknown/absent nodes
      ## is desired. This can be dangerous, see
      ##  * https://www.rabbitmq.com/cluster-formation.html#node-health-checks-and-cleanup
      ##  * https://groups.google.com/forum/#!msg/rabbitmq-users/wuOfzEywHXo/k8z_HWIkBgAJ
      cluster_formation.node_cleanup.only_log_warning = true
      cluster_partition_handling = autoheal
      ## See https://www.rabbitmq.com/ha.html#master-migration-data-locality
      queue_master_locator=min-masters
      ## See https://www.rabbitmq.com/access-control.html#loopback-users
      loopback_users.guest = false
   
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: rabbitmq
  namespace: airflow
spec:
  serviceName: rabbitmq
  replicas: 1
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      serviceAccountName: rabbitmq
      terminationGracePeriodSeconds: 10
      containers:        
      - name: rabbitmq-k8s
        image: rabbitmq:3.7
        volumeMounts:
          - name: config-volume
            mountPath: /etc/rabbitmq
          - name: datadir
            mountPath: /var/lib/rabbitmq  
        ports:
          - name: http
            protocol: TCP
            containerPort: 15672
          - name: amqp
            protocol: TCP
            containerPort: 5672
        livenessProbe:
          exec:
            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]
          initialDelaySeconds: 60
          # See https://www.rabbitmq.com/monitoring.html for monitoring frequency recommendations.
          periodSeconds: 60
          timeoutSeconds: 15
        readinessProbe:
          exec:
            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]
          initialDelaySeconds: 20
          periodSeconds: 60
          timeoutSeconds: 10
        imagePullPolicy: Always
        env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name           
          - name: RABBITMQ_USE_LONGNAME
            value: &quot;true&quot;
          # See a note on cluster_formation.k8s.address_type in the config file section
          - name: RABBITMQ_NODENAME
            value: &quot;rabbit@$(MY_POD_NAME).rabbitmq.airflow.svc.cluster.local&quot;
          - name: K8S_SERVICE_NAME
            value: &quot;rabbitmq&quot;
          - name: K8S_HOSTNAME_SUFFIX
            value: '.rabbitmq.airflow.svc.cluster.local'
          - name: RABBITMQ_ERLANG_COOKIE
            value: &quot;mycookie&quot; 
      volumes:
        - name: config-volume
          configMap:
            name: rabbitmq-config
            items:
            - key: rabbitmq.conf
              path: rabbitmq.conf
            - key: enabled_plugins
              path: enabled_plugins
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
      - &quot;ReadWriteOnce&quot;
      resources:
        requests:
          storage: 5Gi 
</code></pre>
<ol start="4">
<li>have an airflow webserver to provide UI support for airflow,we could manage/ track the dag via UI, kubernetes is check it's liveness and readiness on path <code>/admin/</code> and port 8080, we could also deploy ingress-nginx with http basic auth to expose the this web UI out side of kubernetes cluster</li>
</ol>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: airflow-celery-webserver
  namespace: airflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-celery-webserver
  template:
    metadata:
      labels:
        name: airflow-celery-webserver
        app: airflow-celery-webserver
    spec:
      containers:
      - name: airflow-celery-webserver
        image: docker.io/email2liyang/docker-airflow:1.10.2
        imagePullPolicy: Always
        env:
        - name: LOAD_EX
          value: &quot;y&quot;
        - name: EXECUTOR
          value: Celery
        - name: AIRFLOW_MYSQL_DB_HOST
          value: mysql
        - name: AIRFLOW_MYSQL_DB_PORT
          value: &quot;3306&quot;
        - name: FERNET_KEY
          value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
        - name: AIRFLOW__CELERY__BROKER_URL
          value: amqp://guest:guest@rabbitmq:5672
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__EXECUTOR
          value: CeleryExecutor
        readinessProbe:
          httpGet:
            path: /admin/
            port: 8080
          initialDelaySeconds: 8
          timeoutSeconds: 10
        livenessProbe:
          httpGet:
            path: /admin/
            port: 8080
          initialDelaySeconds: 8
          timeoutSeconds: 10
        ports:
        - name: webserver
          containerPort: 8080
        command: [&quot;/entrypoint.sh&quot;]
        args: [&quot;webserver&quot;]
---
apiVersion: v1
kind: Service
metadata:
  name: airflow-celery-webserver
  namespace: airflow
spec:
  type: NodePort
  ports:
  - name: webserver
    port: 8080
    targetPort: webserver
  selector:
    app: airflow-celery-webserver        
</code></pre>
<ol start="5">
<li>have an airflow scheduler deployed in kubernetes to schedule tasks into different worker nodes, each cluster could only have 1 scheduler for now, we use ps command to do the liveness and readiness check</li>
</ol>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: airflow-celery-scheduler
  namespace: airflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-celery-scheduler
  template:
    metadata:
      labels:
        name: airflow-celery-scheduler
        app: airflow-celery-scheduler
    spec:
      containers:
      - name: airflow-celery-scheduler
        image: docker.io/email2liyang/docker-airflow:1.10.2
        imagePullPolicy: Always
        env:
        - name: LOAD_EX
          value: &quot;y&quot;
        - name: EXECUTOR
          value: Celery
        - name: AIRFLOW_MYSQL_DB_HOST
          value: mysql
        - name: AIRFLOW_MYSQL_DB_PORT
          value: &quot;3306&quot;
        - name: FERNET_KEY
          value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
        - name: AIRFLOW__CELERY__BROKER_URL
          value: amqp://guest:guest@rabbitmq:5672
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__EXECUTOR
          value: CeleryExecutor  
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - ps -ef | grep scheduler | grep -v &quot;grep&quot;
          initialDelaySeconds: 8
          timeoutSeconds: 10
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - ps -ef | grep scheduler | grep -v &quot;grep&quot;
          initialDelaySeconds: 8
          timeoutSeconds: 10
        command: [&quot;/entrypoint.sh&quot;]
        args: [&quot;scheduler&quot;]
</code></pre>
<ol start="6">
<li>have an airflow worker deployed into the kubernetes cluster , we use ps command to do the liveness and readiness check.</li>
</ol>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: airflow-celery-worker
  namespace: airflow
spec:
  replicas: 2
  selector:
    matchLabels:
      app: airflow-celery-worker
  template:
    metadata:
      labels:
        name: airflow-celery-worker
        app: airflow-celery-worker
    spec:
      containers:
      - name: airflow-celery-worker
        image: docker.io/email2liyang/docker-airflow:1.10.2
        imagePullPolicy: Always
        env:
        - name: LOAD_EX
          value: &quot;y&quot;
        - name: EXECUTOR
          value: Celery
        - name: AIRFLOW_MYSQL_DB_HOST
          value: mysql
        - name: AIRFLOW_MYSQL_DB_PORT
          value: &quot;3306&quot;
        - name: FERNET_KEY
          value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
        - name: AIRFLOW__CELERY__BROKER_URL
          value: amqp://guest:guest@rabbitmq:5672
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__EXECUTOR
          value: CeleryExecutor  
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - ps -ef | grep celeryd | grep -v &quot;grep&quot;
          initialDelaySeconds: 8
          timeoutSeconds: 10
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - ps -ef | grep celeryd | grep -v &quot;grep&quot;
          initialDelaySeconds: 8
          timeoutSeconds: 10
        command: [&quot;/entrypoint.sh&quot;]
        args: [&quot;worker&quot;]
</code></pre>
<ol start="7">
<li>optionally, have an airflow celery flower deployed into the cluster to monitor celery work's health, we use path <code>/</code> and port 5555 to do the readiness and liveness check. with celery workers managed by kubernetes, you may see some celery workers come and go, this is  natture in kubernetes world.</li>
</ol>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: airflow-celery-flower
  namespace: airflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-celery-flower
  template:
    metadata:
      labels:
        name: airflow-celery-flower
        app: airflow-celery-flower
    spec:
      containers:
      - name: airflow-celery-flower
        image: docker.io/email2liyang/docker-airflow:1.10.2
        imagePullPolicy: Always
        env:
        - name: LOAD_EX
          value: &quot;y&quot;
        - name: EXECUTOR
          value: Celery
        - name: AIRFLOW_MYSQL_DB_HOST
          value: mysql
        - name: AIRFLOW_MYSQL_DB_PORT
          value: &quot;3306&quot;
        - name: FERNET_KEY
          value: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
        - name: AIRFLOW__CELERY__BROKER_URL
          value: amqp://guest:guest@rabbitmq:5672
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: db+mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          value: mysql://airflow:airflow@$(AIRFLOW_MYSQL_DB_HOST):$(AIRFLOW_MYSQL_DB_PORT)/airflow
        - name: AIRFLOW__CORE__EXECUTOR
          value: CeleryExecutor
        readinessProbe:
          httpGet:
            path: /
            port: 5555
          initialDelaySeconds: 8
          timeoutSeconds: 10
        livenessProbe:
          httpGet:
            path: /
            port: 5555
          initialDelaySeconds: 8
          timeoutSeconds: 10
        ports:
        - name: flower
          containerPort: 5555
        command: [&quot;/entrypoint.sh&quot;]
        args: [&quot;flower&quot;]
---
apiVersion: v1
kind: Service
metadata:
  name: airflow-celery-flower
  namespace: airflow
spec:
  type: NodePort
  ports:
  - name: flower
    port: 5555
    targetPort: flower
  selector:
    app: airflow-celery-flower    
</code></pre>
<p>now we could achieve the goal to manage the celery worker by just one command, if we want  20 celery workers, we could simply execute command <code>kubectl scale --replicas=20 deployment airflow-celery-worker -n airflow</code> , then you will get 20 celery works in airflow cluster. I will discuss more on how to organize airflow project in <a href="https://www.vipmind.me/programing/python/organize-airflow-project-in-an-efficient-way.html">next article</a>.</p>
    </div>
  </div>
</div>

<div id="disqus_thread" class="max-width-md"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://vipmind-me.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</body>
</html>
