<!DOCTYPE html>
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-147718386-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-147718386-1');
</script>


  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <title>Mind - Deploy pySpark jobs into kubernetes with python dependencies</title>
  
    
  
  <meta name="description" content="Recently I spend some time on pySpark to see if it could help our team to resolve some of the problems we are facing on apache airflow. airflow is a great tool for scheduling, triggering and monitoring etl jobs, but itself is not a data processing tool, we are trying to use apache airflow to act as both scheudling tool">
  
  
  
  <link rel="shortcut icon" href="../../images/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="../../images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../images/favicon/favicon-16x16.png">
  <link rel="manifest" href="../../images/favicon/site.webmanifest">
  <link rel="mask-icon" href="../../images/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  
  <link href='../../styles/main.min.css?v=1624884200978' rel='stylesheet' type='text/css'>
  
  <style></style>

  
<script>
  var doc_layout_plugin = {
    enabled: true
  };
</script>
</head>
<body>


<script async type="text/javascript" language="javascript" src="../../plugins/plugins.nocache.js?v=1624884200978"></script>
<iframe src="javascript:''" id="__gwt_historyFrame"
        style="position:absolute;width:0;height:0;border:0" tabIndex="-1"></iframe>
<!--[if lt IE 8]>
    <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
<![endif]-->

<div class="nav b5">
  <div class="nav__content max-width-md">
    <div class="nav__content__left">
      <div class="nav__header">
        <a class="logo" href="../../index.html" title="Home">
          <img class="logo__img" src="../../images/logo.png" alt="VIP">
          <span class="logo__name">Mind</span><span class="superscript">me</span>
        </a>
        <label id="menu-icon" class="nav__menu-toggle" for="menu-toggle-1">
          <svg class="nav__menu-icon" viewBox="0 0 448 512"><rect y="398" width="448" height="36"/><rect y="78" width="448" height="36"/><polygon points="6 238 0 238 0 244 0 256 0 274 448 274 448 256 448 244 448 238 442 238 6 238"/></svg>
        </label>
      </div>
      <input id="menu-toggle-1" class="hide" type="checkbox">
      <div id="nav-menus-main" class="nav__menus">
        <a href="../../programing.html">Programing</a>
        <a href="../../infra.html">Infrastructure</a>
        <a href="../../data.html">Data Processing</a>
        <a href="../../life.html">Life is like chocolate</a>
      </div>
    </div>
    <input id="menu-toggle-2" class="hide" type="checkbox">
    <div id="nav-menus-right" class="nav__content__right">
      <a href="../../about.html">About</a>
    </div>
  </div>
</div><div id="crumb-bar" class="cbb b6">
  <div class="cbb__content max-width-md">
    <div class="left-part">
      <div class="breadcrumb-item">
        <a href="../..//">Home</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>
      <div class="breadcrumb-item">
        <a href="../.././toc.html">Table of Contents</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>

      <div class="breadcrumb-item">
        <a href="../../data.html">Data</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>
      <div class="breadcrumb-item">
        <a href="../spark.html">Spark</a>
        <img class="icon-arrow" src="../../images/icon-arrow-right-gray.svg" alt="icon arrow right gray">
      </div>
      <div class="breadcrumb-item">
        <span>Deploy pySpark jobs into kubernetes with python dependencies</span>
      </div>

    </div>
    <div class="right-part">
      <div class="search-field">
        <img class="icon-search" src="../../images/icon-search-gray.svg" alt="icon search gray">
        <input id="breadcrumb-search-box" class="search-box" type="search" placeholder="Search... (double â‡§)">
      </div>
      <img id="back-to-top" src="../../images/icon-top-darkgray.svg" title="Back to Top" alt="icon top darkgray">
    </div>
  </div>
</div><div class="c-title b7">
  <div class="c-title__content max-width-md">
    <h1>Deploy pySpark jobs into kubernetes with python dependencies</h1>
    <div class="meta">
      <img class="avatar" src="../../images/default-avatar.jpg" alt="Ivan" title="Ivan">
      <a class="author">Ivan</a>
      <span class="meta-divider">/</span>
      <span class="updated">26 Sep 2019 19:50:36</span>
    </div>
  </div>
</div><div class="c-article b8">
  <div class="c-article__content max-width-sm">
    <div id="article-body" class="article-body">
      <p>Recently I spend some time on <a href="https://spark.apache.org/docs/latest/api/python/index.html">pySpark</a> to see if it could help our team to resolve some of the problems we are facing on <a href="https://airflow.apache.org/">apache airflow</a>. airflow is a great tool for <a href="https://www.vipmind.me/infra/kube/deploy-apache-airflow-cluster-into-kubernetes.html">scheduling, triggering and monitoring</a> etl jobs, but itself is not a data processing tool, we are trying to use apache airflow to act as both scheudling tool and data processing tool, here comes a problem, we have only one airflow cluster anytime airflow get redeployed, some running jobs got a chance to get interrupted. there are some ways to overcome this issue, but still running differnt jobs on the same cluster is not a good idea. it's ideal to have each data processing job running on it's dedicated cluster and isolated from each other. Spark support to deploy to kubernetes out of box from v2.3, and we could create dedicated spark cluster for any single spark job. as our team prefer python as the first development language , so pySpark looks like a perfect tool to look into.</p>
<p><figure><img src="../../images/pySpark-on-eks.png" alt="pySpark-on-eks"></figure></p>
<h2 id="pyspark-image-1">pySpark image</h2>
<p>spark team does not provide pre-build docker image for spark, but it provide a <a href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh">docker-image-tool.sh</a> to build an push spark images to docker registory, I've built an image <a href="https://cloud.docker.com/u/email2liyang/repository/docker/email2liyang/spark-py">email2liyang/spark-py</a> and publish it to github for public use, we could also use this image as a base image and custome spark-py image to meet team's own need. for example, install additional software packages or install some python dependencies, I will take about this later.</p>
<h2 id="pyspark-project-setup-2">pySpark project setup</h2>
<p>I use pipenv to manage my python project dependencies,you could check <a href="https://www.vipmind.me/programing/python/set-up-python-development-env-with-pyenv-and-pipenv.html">this article</a> for how to use pipenv for details. there could  be 2 main folders in the project</p>
<ul>
<li>vip_udfs: hold the business logic for all the jobs, different logic could be separated in different python package, it mostly expose as  <a href="https://docs.databricks.com/spark/latest/spark-sql/udf-python.html">user defined function(udf)</a>, the udf could be tested as standard pthon function as well</li>
<li>vip_jobs: hold the main pySpark jobs , it in charge of create the spark job and set the dependency path etc. it will call udf defined in package vip_plugins</li>
</ul>
<h2 id="python-dependency-management-3">python dependency management</h2>
<p>the python dependendcies is the python code which is expected to be in each worker's runtime python path, so that the pySpark job could invoke the funcitons provided from the dependencies. there are 3 kinds of dependencies</p>
<ul>
<li>the 3rd party python dependencies<br />
we could use pipenv to install the 3rd party python dependencies into local env and expose them via <code>pipenv lock -r &gt; requirements.txt</code>, when we build custom docker image based on spark-py, we could install these dependencies.</li>
<li>the UDF defined in folder <code>vip_udfs</code><br />
these are the user defined functions which support the main pySpark jobs,these python module could be packaged as a zip file and send to a shared volume in the spark cluster, in my case I've provision an <a href="https://aws.amazon.com/efs/">AWS EFS</a> and mout it as shared volume across all the pySpark worker pods, let's assume <code>vip_udfs</code> is packaged as vip_udfs.zip and copied to path <code>/var/efs/pyspark/job-station/vip_job1/vip_udfs.zip</code><br />
to make the spark worker could access this type of python dependencies, in our pySpark job, we need to write like below</li>
</ul>
<pre><code>spark_session = SparkSession \
        .builder \
        .appName(&quot;vip_job1&quot;) \
        .getOrCreate()
spark_session.sparkContext.addPyFile('/var/efs/pyspark/job-station/vip_job1/vip_udfs.zip')
</code></pre>
<ul>
<li>the 3rd party jars<br />
spark is written in <a href="https://www.scala-lang.org/">Scala</a> and running on <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">JVM</a>, so the core spark runtime is running in <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">JVM</a> , we could also utilise some 3rd party java lib to help us on some tasks in the flow. e.g: load data from mysql through mysql jdbc. to do this , we could copy <code>mysql_mysql-connector-java-5.1.44.jar</code> into a shared location, e.g: <code>/var/efs/pyspark/jars/mysql_mysql-connector-java-5.1.44.jar</code> and in spark-submit, we could delcaer the additional jar via <code>--jars</code> options.</li>
</ul>
<h2 id="sumbit-pyspark-job-into-kubernetes-4">sumbit pyspark job into kubernetes</h2>
<p>below is the pySpark architecture on kubernetes, we could use spark-submit utility to submit the pySpark job into kubernetes cluster, it will create a driver pod first, and the driver pod will also communicate with api server to create executor pod as defiend, the driver pod also in charge of all the job pod's life cycle, we only need to manage the driver pod.</p>
<p><figure><img src="../../images/how-pyspark-works.png" alt="how-pyspark-works"></figure></p>
<p>below is a sample command to submit a pySpark job into kubernetes cluster with explanation on each option</p>
<pre><code>spark-submit \
    # the kubernetes api point, we need to config kubectl to be able to access this
    --master k8s://https://xxxxx.aa.us-east-1.eks.amazonaws.com:443 \
    # declare we use cluster mode 
    --deploy-mode cluster \
    # the 3rd party jar which could be access from all the spark runtime
    --jars /var/nfs/pyspark/jars/mysql_mysql-connector-java-5.1.44.jar \
    # the udf files which could be access from the driver node
    --py-files local:///var/efs/pyspark/job-station/vip_job1/vip_udfs.zip \
    # spark could run on under dedicated service account
    # an rbac config is needed for this service account to access kubernetes api
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sc \
    # the spark could run all the spark job in it's own namespace
    --conf spark.kubernetes.namespace=spark \
    # when we build custom docker image
    # it could be stored in a private docker registery e.g:gcr
    # this secret name is used to pull the image from private docker registry
    --conf spark.kubernetes.container.image.pullSecrets=vip-mind-docker-registry \
    # the custom pyspark image to pull from docker registry
    --conf spark.kubernetes.container.image=us.gcr.io/vip-mind/my-pyspark:latest \
    # specify how offten you want to pull the new image
    --conf spark.kubernetes.container.image.pullPolicy=Always \
    # the job name 
    --name vip_job1 \
    # how many executors you want the driver to create
    # in this case the driver pod will create 10 executor pods to process the data
    --conf spark.executor.instances=10 \
    # currently spark support both python 2 and python3
    # but spark will drop python 2 in a short time
    # so it's recommended to use python 3
    --conf spark.kubernetes.pyspark.pythonVersion=3 \
    # we could mount additional pv into the driver pod, e.g: the efs
    # so that driver could access the shared efs volume
    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc.mount.path=/var/efs \
    # do we mount the volume as readOnly?
    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc.mount.readOnly=false \
    # which pvc we should claim from for the efs
    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc.options.claimName=efs \
    # we could mount additional pv into the driver pod, e.g: the efs
    # so that driver could access the shared efs volume
    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc.mount.path=/var/efs \
    # do we mount the volume as readOnly?
    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc.mount.readOnly=false \
    # which pvc we should claim from for the efs
    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc.options.claimName=efs \
    # we could also assign the spark to run on specific ec2 nodes   
    --conf spark.kubernetes.node.selector.beta.kubernetes.io/instance-type=m5.large \
    # we could also set the how much cpu or memory when launch the executor pod
    --conf spark.kubernetes.executor.request.cores=300m \
    # the py spark main job file
    local:///var/efs/pyspark/job-station/vip_job1/main.py
</code></pre>
    </div>
  </div>
</div>

<div id="disqus_thread" class="max-width-md"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://vipmind-me.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</body>
</html>
